
URL reference: https://medium.com/@perfectscale/kubernetes-node-huge-nodes-vs-small-nodes-95f4ad0600e0
DateReviewed: 2025-02-01
Description: This article tackles an interesting Cloud question - what is the best node size, small or large, for your clusters? We dive right in, and they go through some background around nodes - what they are, components, ways to identify them (labels and annotations). Then we get to the meat, node allocation and capacity, where it takes about CPU numbers, how much memory and how many pods it might support. They also cover condition status and info, which allow us to dig into metrics around a node. They talk a bit about large nodes (32CPU 128GB memory) and small nodes (2CPU 2GB memory) with some other examples. The note small nodes are cheaper, but you'll need more of them. We then dig into analysis, looking at cost efficiency, simplified management, lower API server load, improved resource utilization - all favoring large nodes. In contrast, huge nodes have downsides, like risk of outages elevated, more downtime for upgrades, resource wastage for small workloads. Then then pivot to small nodes, nothing they are flexible, resilient and efficient in allocation for various workloads. Downsides here include more nodes to manage, more network overhead, and idle resource waste. Each of these points comes with a para of discussion to flesh things out. They then have pointers for choosing which size of node and when. Lastly, they pitch their product - PerfectScale, which helps to choose an optimal node size.
BottomLine: Great discussion of Kubernetes node sizes in the cloud and how to choose
==
URL reference: https://thenewstack.io/build-an-open-source-kubernetes-gitops-platform-part-1/
DateReviewed: 2025-02-02
Description: We're going to explore using Open source to build a Kubernetes GitOps platform in this article. We start off by talking about tools to help you with your K8s cluster, and they note there are lots of them, and I think they are going to help us navigate them here. First, we have to choose a cloud, and they frame the choice as between hyperscalers and simpler IaC offerings. They do mention that many tools are cloud agnostic, and name drop Argo, cert-manager and a few others. They do mention both hybrid or multicloud, and caution that tech like ingress and secret managers can create a challenge in these instances. Next, they say to pick a Git provider, and talk about the two biggest players - GitHub and GitLab. Third up is Platfrom domain and DNS, which they mention Cloudflare for. Fourth up is defining your IaC, and they mention DataDog and Terraform, but talk about a few others. Step 5 is picking your GitOps engine, and mention Argo CD and Flux CD as options. Finally the last step is to define your mgmt pillars, and they describe the loadout they use. They have a follow on post to continue the journey.
BottomLine: Great discussion of options around building a GitOps pipeline on Kubernetes
==
URL reference: https://blog.devops.dev/the-ultimate-guide-to-ephemeral-volumes-in-kubernetes-8983f0395009
DateReviewed: 2025-02-03
Description: While this article is about ephemeral volumes, it surprised me for both the length and depth, in a good way. They have a tiny intro and an AI slop picture, where all of the bits - Kuberntes, pods, storage and volumes are depicted by cabinets? Anyways, we then jump into what are ephemeral vols, which are storage areas tied directly to a pod being alive -- once the pod is destroyed, all of the ephemeral vols are too. They give some characteristics, and then contrast it with persistent volumes, which are long-lived storage areas that persist between pod starts. There are a few types we go through, including emptyDir, configMap, secret and downwardAPI, with explainations of each. We then do an overview of the process of building and using an ephemeral volume, and continue into a bit of hands on, where we are shown the building of a YAML file that has a volume in it, with explaination. We then move on to the mounting process, where an ephemeral volume is mounted into a pod for use, with YAML and explaination. They then go through a bunch of the ephemeral types, use cases, and even a step by step process to do it as a walkthrough. Excellent article.
BottomLine: Excellent explaination of Ephemeral volumes on Kubernetes with a full walkthrough
==
URL reference: https://www.solo.io/blog/ambient-mesh-mtls-for-all-traffic-even-within-the-same-node
DateReviewed: 2025-02-04
Description: So this article we'll tackle Ambient mesh, which is a mTLS provider. Our first bit of intro lets us know this is a sidecar-less approach, and informs us that even intra-node comms are encrypted, meaning two containers running on the same pod still have encryption between them. We then dig into ztunnel a bit, which is how this all works - ztunnel grabs, encrypts and forwards the traffic, and then does the revese at the destination. They further explain that this lives inside the CNI, which is what makes it transparent. By default it catches all traffic, but you can add policies to change what gets encrypted or not, and with what identities. They include a diagram and a link to a vid talk that goes through it.
BottomLine: Good article talking about using mesh to encrypt all Kubernetes traffic
==
URL reference: https://stripe.com/blog/how-stripes-document-databases-supported-99.999-uptime-with-zero-downtime-data-migrations
DateReviewed: 2025-02-05
Description: This article is about a five-nines uptime that Stripe strives for. They are using something they call DocDB on top of MongoDB, which is an enterprise level extension to help with uptime. They talk a bit about approaches, then settle into their arch - their API, backed by DB proxies, writing to specific shards and maintaining consistency with a metadata service and a change data capture pipeline. They walk through their 5 steps and a couple more diagrams, but it's actually hard for me to understand their approach clearly. They use NoSQL, which maybe makes sense as they are facilitating by not the end point of transactions, but it seems like they do LOTS of work to do the things you get for free with a good SQL DB?
BottomLine: Interesting read of how Stripe manages to get amazing uptime
==
URL reference: https://dzone.com/articles/kubernetes-cluster-with-pod-security-admission
DateReviewed: 2025-02-06
Description: Our article will look at how Pod Admission security functions. We start with a short intro and some background into general Kubernetes security, like access control and auth, network polices among others. Pods are the smallest deployable units, so we have to have a way to secure them. They first introduce levels and abilities, so priv, baseline and restricted then warn, audit and enforce, and then we get into how it works. The API request to create or update a pod is captured, and the pod is compared against security standards for compliance. In warn it issues a warning for non compliance, in audit it logs violations and in enforce it stops the API call from creating or updating the pod. We then go through a brief walkthrough to set up the security, including CLI and YAML files and tests to show it's working as intended. To finish off, they mention some best practices.
BottomLine: Decent intro and walkthrough for Pod Security Admission for Kubernetes
==
URL reference: https://blog.devops.dev/complete-guide-to-kubernetes-port-forwarding-a9eb1237c4c7
DateReviewed: 2025-02-07
Description: Okay, so this article is better than I thought at first, but is a guide to Kubernetes port forwarding. We start with an AI slop-look image, and most of the article is in point form, so there is that. They explain why we need port forwarding, and the syntax needed to get things working. We then have a control flow diagram, and a step by step of how this actually works. We also have a small diagram explaining how traffic flows between the pod port and the local port. They then work through some use cases, but they are more like a hands on walk through. They set up testing services/apps locally and then connecting to a DB for development. In each they lay out all of the CLI commands, and have lots of screenshots and file/CLI views to show how things should look. Actually a great look at port forwarding in Kubernetes.
BottomLine: Good deep intro to port forwarding in Kubernetes
==
URL reference: https://medium.com/@osmarrleao/pacman-on-the-cloud-how-to-deploy-a-kubernetes-application-on-zcloud-using-a-fully-integrated-eks-263ca1a6fb65
DateReviewed: 2025-02-08
Description: Okay, for this article we're going to have a bit of fun - they are going to describe how to deploy Pacman with a new cloud system. We start with a ref to a repo, then the architecture - MongoDB for the high scores and internal user data, and another Pod to run the game. First we fire up the namespace we'll put all of this in, and then create the PVC for the storage. We then walk through the Pacman pod an the CLI to fire everything up. Then go through the MongoDB YAML and CLI bits, and check that the PV is right. Once everything is up, we have two pacman pods and a MongoDB pod up and ready. And we are all ready to play! Fun walkthrough.
BottomLine: Neat walkthrough of a new cloud service using a Pacman game on Kubernetes
==
URL reference: https://thenewstack.io/root-out-vulnerabilities-in-github-as-you-merge-code-changes/
DateReviewed: 2025-02-09
Description: Our article today is more of a discussion than a hands on, with an app sec company trying to tap into the AI area. They lament that "shift left" has put more burden on devs without giving them tools, and that is where their company, DryRun, fits in. They use something called Natural Language Code Policies to help identify true threats and minimize false positives. They claim their tool runs fast, in as little as 10 seconds in places. It uses context to make assertions in near real time. They compare themselves to other tools, which are genreally static in their approaches, and pattern based. They include a pretty diagram showing "risky" code changes, without explaining how they surface them. They claim that rather than obscure links to websites, their tool give real guidance. For now, if you want to run their tool you have to use GitHub - the GitLab version is on the way. Okay, my take - it sounds interesting, but the devil with this is in the details. They handwave and say 80-90% is doable by their inbuilt rules, but how hard is it to build a ruleset or a custom rule? Also, is their in-house guidance going to be better than a world-class website in explaining a common error?
BottomLine: New AI app sec tool which is worth giving a read to
==
URL reference: https://8grams.medium.com/longhorn-cloud-native-cloud-storage-solution-from-rancher-for-kubernetes-704e6261660d
DateReviewed: 2025-02-10
Description: In this article they are exploring Longhorn, Rancher's entry in Clould native storage. We start with an intro, where they outline the need for storage in the cloud. We then look more closely at the Kubernetes space - the initial approach of read-only and lightweight containers, which was followed by the needs of things like databases for long-term persistent storage. They now talk about how Longhorn originated as a side project to support Rancher, their take on Kubernetes container management. They mention it became an official project of CNCF, before becoming a dependable storage solution. We finally get to the arch - and there is a diagram, yay! There are four main areas - the managers, the engine, replica pods and snapshots and backups. Each of these is explained in about a paragraph, where the manager is in charge of where things go, the engine is the I/O part for the volumes, and the replicas actually store the data to the physical stores. They also talk about seperation of control and data planes, to segregate security and increase performance. They walk through replication and redundancy, include a diagram and a longer discussion. There are a few more benefits and then they tackle comparisons with other storage engines. The look at Ceph, Portworx and OpenEBS as competition. Finally, they look at how Longhorn integrates with Kubernetes, and where it is strongest. Actually a fantastic intro exploration.
BottomLine: Very interesting intro to Longhorn as a storage option for Kubernetes
==
URL reference: https://medium.com/@rifewang/overview-of-kubernetes-cni-network-models-veth-bridge-overlay-bgp-ea9bfa621d32
DateReviewed: 2025-02-11
Description: Our article is going to explore Kubernetes networking in some specific ways. First, they remind us of the two rules of K8s networking - that a pod can communicate with any other pod w/o NAT, and that agents on a node can comm with all pods on that node. We first start with IPAM, which does IP management for nodes in K8s. They walk through the YAML and do a quick explaination of how it works. They then toe dip into VETH and Bridge, with a diagram explaining how this all works. They do note that things are more complex, and even show an extended frame with all of the embeddings to have traffic flow right. They talk a bit about Overlay as well. They also look at BGP for extended scenarios like hybrid or multi cloud, where you might need to route amoung a few clusters. Overall a decent walkthrough.
BottomLine: A decent touch on many levels of Kubernetes networking
==
URL reference: https://dev.to/vris_jaijongrak_e051f8cf/setup-aws-eks-with-karpenter-with-terraform-1mhd
DateReviewed: 2025-02-12
Description: This article is based off a presentation from a community day for AWS. The author mentions that containers are driven by microservices, which are in turn driven with things like CI/CD. They are going to look at setting things up with AWS using the EKS services. They are going to emphasize security, while still paying attention to cost and performance. They did a demo, where they showed how to do the setup of various resources, and included the presentation slides. A decent intro to using EKS and how to set it up for CI/CD.
BottomLine: Solid intro for using EKS to do CI/CD for Kubernetes
==
URL reference: https://www.suse.com/success/nts/
DateReviewed: 2025-02-13
Description: So, this article purports to talk about migrating a SIEM application to Kubernetes clusters, but at first glance it looks like ad copy, but let's dive in. Ah, it's the story of a client that is using services and an overview of that; I was concerned because there was both an "at a glance" section and a point-form highlights area. Brief intro of the client and then we dive into their need - to build a SOC to provide managed security to customers. They are using Splunk, and decided to use microservices to roll it out easily to customers. They then talk about their journey, starting with Longhorn as a data layer and then moving on to Rancher Prime for their containers as their platform evolved. They talk a bit about the pieces, but it's all very high level. Interesting read but not much practical advice.
BottomLine: Interesting story about building a SIEM on Kubernetes
==
URL reference: https://piotrminkowski.com/2025/01/14/continuous-promotion-on-kubernetes-with-gitops/
DateReviewed: 2025-02-14
Description: This article will touch on a technique called promotion, where we move different builds around into different areas like dev, test and prod. This is done using GitOps in the article, which is usually the way it is done - specifically ArgoCD. They give a bit of background and then we get a diagram showing how things work. They tap a tool called Kargo that does promotion integrated with CI/CD, so on a continuous basis in pipelines. They do explain you can do it in GitOps with a tool called Devtron and provide a link. They do a bit of background with Kargo, explaining how it uses project, stage, freight and warehouse to model the different parts of the process, and promotion to move between the stages. Now we dive in, installing pre-reqs and Kargo itself, then set up the permissions and finally verify the pods are running as expected. They expose the web dashboard with a port-forward, and we're ready to go. They sign in and create a sample app, and then do some tweaking to package the simple app. We craft up a little bit of YAML that is used to let the pipeline know that our app should be monitored, and then we have to set up ApplicationSets for each env -- in this case, test, uat and prod. There is a bit more setup needed, but finally a "push to promote to stage" button shows up in the dashboard. We do so, and it gets deployed to test, and soon after we deploy it to the other two as well; and they are all showing healthy. They then show us how to test the sample app directly. They then show the update process, and note that a promotion of a tag to one env then allows it to be promoted to the next env in the line. Decent walkthrough.
BottomLine: Promotion is demonstrated through pipelines to Kubernetes clusters
==
URL reference: https://dev.to/adzhydra/setting-up-development-testing-and-production-namespaces-in-kubernetes-a-practical-guide-2025-5c2a
DateReviewed: 2025-02-15
Description: This article will go over setting up dev/test/prod namespaces on Kubernetes. They start by explaining that while the default namespace is fine for small projects and learning, you need more as you get more serious. We go through why it matters, things like security, eliminating conflicts between envs, and having better monitoring and debugging among other things. They go through the simple namespace creation CLI commands, but note that the interesting part is in the configuration. They walk through simple stuff like resource limits and even include a YAML snippet, and then talk about networking from a security perspective, again with YAML snips. They then talk about RBAC - for both networking and access control, the idea is that devs have access to the devel env, where prod is much more tightly controlled. Actually a good article.
BottomLine: Good short article on using Namespaces in Kubernetes for isolation and security with some hands on
==
URL reference: https://medium.com/@khassan9/rethinking-kubernetes-cost-optimization-going-beyond-traditional-methods-e1e584ef0eba
DateReviewed: 2025-02-16
Description: This article is going to help us reduce cost, or so it promises. We start strong, talking about some traditional approaches like rightsizing nodes and maximizing resource allocation. Then, we are introduced to the product pitch, which is for Cloudidr, a product that helps optimize K8s clusters. They use three approaches, and then try to explain why you are paying less, without explaining how it works. So, magic. Just a bad product pitch.
BottomLine: Bad product pitch - no details - probably skip
==
