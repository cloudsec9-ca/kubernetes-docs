URL reference: https://www.infracloud.io/blogs/unlocking-kubernetes-power-with-rke-custer-metallb-rook-ceph/
DateReviewed: 2024-07-01
Description: Blog post on running Rancher Kubernetes engine on a bare metal setup. We start with a walk through what RKE (Rancher K8s engine) is, and how it differs, including better defaults, US Fed. Gov't standards compliance and continuous vulnerability monitoring. They talk also about it's lineage, where it gets simplicity as a K3s derivative, and how they have shifted away from relying on Docker. They note setting up a proper amount of memory and disk space is crucial or you can run into issues with your K8s master. They also note that MetalLB doesn't work with many cloud providers, as they don't support its networking requirements, but it will work with VirtualBox. Next we move onto installing RKE2, which has a laundry list of prereqs and then we have to fetch, install and enable the rke2 service. Next we start the service and confirm its running by checking the logs. We do a couple kubectl commands and ensure our K8s is up. Next, we set up the worker nodes, which includes many of the above steps with the additional step of using the master token to join the existing cluster. Next we install the load balancer and configure it, with a test service to ensure it is working. We then move on to setting up storage with Rook-Ceph. We go through pre-reqs, install, create a storage cluster then make it available. There are a lot of moving parts here, and I think think it would have been better to break it into a few parts; but it is a solid intro to the pieces.
BottomLine: Deploy Rancher Kubernetes to bare metal with LB and Storage
==
URL reference: https://www.redhat.com/en/blog/friday-five-june-28-2024?sc_cid=701f2000000tyBjAAI
DateReviewed: 2024-07-02
Description: This is just a stub for the bigger "State of Kubernetes security in 2024", which RedHat does every year. It does link to a brief blog overview, which is what I'll review here. It gives an executive summary, with three charts and a feeling of what the surveyed participants feel are the tough parts of Kubernetes security today. It's good because it's a real world survey, and not someone's feelings on what should be where. It's a good toe dip.
BottomLine: Overview of the annual State of Kubernetes security.
==
URL reference: https://github.com/bunkerity/bunkerweb
DateReviewed: 2024-07-03
Description: BunkerWeb is a WAF (web application firewall) that is open source. They start with a quick overview, then a seven part segment on why, with each point being about a paragraph. They then do a rundown of the various security features, and finally go through resources, which includes a demo version to test out, a cloud variant, a paid Pro version and consulting services, and a long list of community focused resources and documentation. Looks very interesting.
BottomLine: A new open source WAF with cloud ability called BunkerWeb
==
URL reference: https://medium.com/@davis.angwenyi/how-to-install-grafana-loki-using-helm-e6b1185e5c24
DateReviewed: 2024-07-04
Description: This article discusses installing a comprehensive logging tool called Loki. We start with a diagram and there are a couple in here, which is always a good sign. They start with going through three variations of logging, monolithic, simple scalable and microservice, with a broken out explaination for each variant. It then goes through how the log bits work, tracing down both the write and read path for logging. Now we get into the install - so we add the repo in a helm CLI command, and then drop the YAML configs to be able to run the software. They then go through the YAML in chunks, explaining the various bits and what they do, and some of the options that are available. Once our config is ready, we can actually run the install to get things set up. This is the backend; they also run through how to set up and connect the client to send the logs. Pretty decent setup walkthrough.
BottomLine: Walkthrough install and basic info on the Grafana Loki Kubernetes logging piece.
==
URL reference: https://medium.com/intel-tech/how-to-containerize-your-local-llm-436182cd179a
DateReviewed: 2024-07-05
Description: This article is about using an LLM through an API from a container. They start with a walk-through of how to interact with an LLM and a bit on they why side, and then pose reasonable questions like how would you do this through a web framework. They give a link to the code, and then tackle an important question - why in a container. They talk a bit about using the model from internal or external spots, as well as saying that storing the model external to the container image can save space and make deployments faster. Basically they argue for decoupling the model from the container, to allow each to be updated seperately. You still need to store the model, which can run 26GB. They do point out that by running the model locally, you can tune the output it produces more finely. They then lead us a bit through the front end setup, and how to make it useful to devs and end users. They walk us through the Python code they use, and then show us how to create the Docker image for the container we'll be deploying. Finally they show us how to upload the image to a Registry for deployment.
BottomLine: Decent walk through on setting up an LLM in a container
==
URL reference: https://medium.com/adidoescode/adidas-how-we-are-managing-a-container-platform-1-3-6ce24e75649
DateReviewed: 2024-07-06
Description: This article is about a global scale collection of Kubernetes clusters, so you can get some insights into them working at that scale. This is focused on the apparel company Adidas, which deploys to 5 global areas with between 5 and 30 clusters and one or two regions within each area. They did use code to manage their configs, but it wasn't a globally shared code but rather branches for each region as they have their own idiosyncracies. There were lots of moving parts in this, including maint windows, different configs managed through the CI/CD process, with up to 50 configs that it was managing. Very interesting background and interesting items to understand.
BottomLine: Peek into a global scale Kubernetes deployment over many clusters
==
URL reference: https://www.itsecuritynews.info/portainer-open-source-docker-and-kubernetes-management/
DateReviewed: 2024-07-07
Description: This is a blurb that points to an actual article, so I'll keep this short. Portainer sounds like an interesting software package, able to manage Docker, Swarm, K8s and ACI envs with a GUI. This stump of an article, however, is not.
BottomLine: Stub which is actually not very helpful on Portainer - skip
==
URL reference: https://itnext.io/run-your-kubernetes-cluster-on-bare-metal-with-cilium-cni-part-1-e88028800d90
DateReviewed: 2024-07-08
Description: This article will help set up an on-prem home lab Kubernetes setup. We dive right in, first with the requirements, and they start with how the network has to be configured. Next up they explain the hypervisor, and both of these elements are tackled in detail. They then lay the background of our eventual target, with a pretty diagram to tie it together. Next up is the install, and they explain the hardware we'll need to proceed, before running us through the CLI commands to get everything on track. They then set up CRI, and explain with a breif explanation what that does. They then show us how to set up on two architectures, one ARM and one x86_64, depending on what CPU you'll be using. We go through a wall of CLI text, which is actually getting K8s to install and run, both for the masters and for the workers, and we even check to make sure it works. Next up is getting Cillium working, and again there is a large wall of text. My thoughts? It's a LOT of config, and I think it would help if there was a bit more "we are doing this for this reason", so people understand. But it's good to support getting a homelab up.
BottomLine: Walkthrough of getting a Kubernetes homelab up with Cilium
==
URL reference: https://it-notes.dragas.net/2024/07/04/from-cloud-chaos-to-freebsd-efficiency/
DateReviewed: 2024-07-09
Description: This article is a dive into a Kubernetes deployment which was complex, and the journey to simplify things. The choice here was to move from a cloud solution to a physical colo. On this base, they used various tech like FreeBSD jails to achieve things like virtualization and isolation. They also used ZFS, which is akin to a Logical Volume Manager on Linux. They tell a story about a project that got deleted, but was restored in a few minutes from the ZFS backups he had built. There is a lessons learned section, but to me it is a bit vague -- our author talks about the previous setup and the current one, but there are no benchmarks or hard numbers, just talk of autoscaling and odd development choices. There is another story of a possible cryptominer which was causing load spikes on their old setup, due to an exploit. My take is that different projects have different needs, and while some teams could definately jump environments, many teams could not.
BottomLine: Interesting article on moving from the Kubernetes cloud to FreeBSD jails
==
URL reference: https://medium.com/@ujjwalsapkota005/exploring-kubernetes-services-clusterip-nodeport-externalname-and-loadbalancer-22553b33910f
DateReviewed: 2024-07-10
Description: This article explores different ways to allow traffic to get to your services through the network. They review ClusterIP, NodePort, ExternalName and LoadBalancer. There isn't too much background so lets dive in. There are a number of diagrams, even if the text doesn't really reference them. We start with ClusterIP, which they say is local to the cluster, and good for internal services. They do show a YAML file and both how to deploy and how to create a ClusterIP on the CLI. Next up is NodePort, which exposes a port on each nodes static ports, and allows explicit external traffic. This too has YAML and CLI bits. We move onto to ExternalName, which maps a service to a DNS name, which allows services to reach an external service. We get YAML for the config. Last up is LoadBalancer, which routes traffic to one or more pods. We get YAML and CLI for this one. They do a few examples of various configs using some of these resources. An interesting walkthrough, but lacks a bit of explanation.
BottomLine: Good basic cover of various mainly inbound Kubernetes constructs
==
URL reference: https://bughunters.google.com/blog/6669874749636608/securing-the-container-world-with-policies-acjs-and-ctrdac
DateReviewed: 2024-07-11
Description: This blog is the introduction of two new controllers to the Kubernetes ecosystem. They are acjs, the Admission Controller with JavaScript, and ctrdac, the Containerd Admission Controller. They start with a section on a recap of Admission controllers, explaining a bit about what they are and how they work in a few paragraphs. We also get a diagram of how they work inside and even outside of K8s clusters. We then have sections for each new controller, where they go through the details. Following that, they go through 3 examples so you can understand how they work. Actually a good intro to new controllers.
BottomLine: Introduction of 2 new Kubernetes admission controllers
==
URL reference: https://medium.com/@harmandiaz023/navigating-kubernetes-cost-management-challenges-the-ultimate-guide-53d018905529
DateReviewed: 2024-07-12
Description: We start with Kubernetes good, but costs not managed are bad; so lets dive in. We next talk about cost challenges in K8s, which include bad resource allocation and optimization, complex pricing, multi-cloud envs, governance and compliance, estimation and budgeting. Each of these items gets a paragraph or so of coverage, so we know what is being done wrong. Next we look at cost mgmt strategies, and these include optimizing resource allocation, choosing the right cloud provider, and cost monitoring and reporting. I've seen a few of these, and this one seems lighter than others, and doesn't really say anything different or new.
BottomLine: Rehash of previous pieces on Kubernetes cost management
==
URL reference: https://www.computerweekly.com/feature/Kubernetes-at-10-When-K8s-won-and-life-now-as-a-surly-teenager
DateReviewed: 2024-07-13
Description: With Kubernetes turning 10, it is now time to reflect a little, and in that context, the person interviewed here believes K8s is in the "surly teenager phase. They start with talking about the software landscape when K8s launched, which included Docker Swarm and Apaache Mesos. We dive a bit into the interviewees background, which is in distributed DBs by way of Cassandra, and Spark, which fit well with Mesos. He says his lightbulb moment of when K8s "won" was when an enterprise Mesos vendor announced K8s support. He does mention that K8s started with poor storage support, which caused DB people to avoid it. The catalyst here was the introduction of StatefulSet support, which allowed persistent storage without 3rd party extras. The addition of operators cemented K8s position as it allowed control over how services were added and removed. They then talk a bit about the proliferation of various operators, and the shakeout to get some core ones going. This, they explain, helped drive cloud native adoption. He says K8s is still growing and evolving, while it has a solid base to grow on. A decent piece and good retrospective on K8s.
BottomLine: Ten year of Kubernetes through the eyes of a distributed DB guy
==
URL reference: https://itnext.io/automate-kubernetes-with-shell-operator-1ae5b50408ae
DateReviewed: 2024-07-13
Description: We start by reflecting on developing a Kubernetes operator usually means knowing K8s and Go well. To turn that on it's head, a group developed the "shell operator", which allows use of scripting langs like Bash, Python and Perl. To understand what this is all about, we're going to follow the fictional goal of getting real-time pod monitoring working. First, we start with a diagram which shows what we'll end up with, and it looks pretty complex. We now start with a discription of how the shell operator works -- namely, by watching for specific events. When an event it is watching for happens, it then executes a script in response, and typical events it looks for are creation, deletion or updates of resources. Now that we know the overall mechanism, next up is using the features, so first we go through them. First up is event handling, and this is how the operator finds out about things. Next is script execution, and this is how the operator responds to events, which can be written in a variety of languages. It runs scripts out of the hooks directory, which ensures flexibility; and you don't have to understand all of the underpinings to automate K8s. In the monitor, they use Terraform with a local Kind cluster along with crossplane to message Slack. We then see an example bash script to notify Slack on pod creation. We're then taken through setting up a Docker image with the shell operator, and then building and pushing this image to a an image repository. They even go through the needed RBAC to get the permissions right as YAML. Next up is the YAML to deploy the shell operator, along with the CLI to deploy the pieces. Now we should see create and delete request show up in Slack. Overall, it's a good intro.
BottomLine: Using the Shell Operator to automate Kubernetes with Python or Bash
==
URL reference: https://github.blog/changelog/2024-06-25-artifact-attestations-is-generally-available/
DateReviewed: 2024-07-14
Description: This is a blog article on Artifact attestatations. The idea is to create a way to build a link between what authors build and the actual software you are using by using a signed authority which isn't able to be tampered with. We get two endorsements, and then an actual walkthrough of what is needed to activate this - in GitHub actions, you add a snip that references your code and pulls in the attest-build-provenance action, and then run it with a CLI command as part of your build process. The announcemnt also says they've released a controller that checks attestations, so you can see that you are using secured software.There is also a video to take you through the process. This is an interesting approach to security, esp. for items that are more than one dependency level deep.
BottomLine: Blog on using attestations in your builds on GitHub to secure your Kubernetes software
==
URL reference: https://securityboulevard.com/2024/07/securing-kubernetes-the-risks-of-unmanaged-machine-identities/
DateReviewed: 2024-07-14
Description: Containers and microservices change the way we build apps, but because of the big shift things like Kubernetes open up new attack surfaces. By spinning up (and down) containers on demand, this makes security more challenging. And apparently every pod could be a gateway for attackers, so each should be protected with the highest level of security. It seems existing tools are not well suited to the short-lived nature of some pods. I pause here, as this is overly hard -- some pods are long lived, and not every pod is a great security risk. We should be vigilant, but this take is too harsh. They now circle back to the basics, including the four C's, as a starting point. (Cloud, cluster, container and code). We then get into a bit about IAM and accounts, before them actually making some good points. They mention lack of visibility, no centralized root of trust, disconnected processes and team silos, ops bottlenecks and lack of control as issues, mainly for manual processes. This is a good observation, and there are a few paragraphs in each item to back things up. Our wrap paragraph is our ad bit, for AppViewX which helps with all of this.
BottomLine: A few late good points but some bad rattling in the opening mar this examination of Kubernetes security
==
URL reference: https://allthingsopen.org/articles/security-posture-and-standard-configurations-across-kubernetes-clusters
DateReviewed: 2024-07-15
Description: This is a short YouTube interview with a written summary on various DevOps pieces. The interview subject did a talk on Pepr, which enhances your security posture; but he doesn't explain that much further. He does talk a bit about configuration challenges, and strongly advocates for standardizing the approach no matter where your cluster is running, so all levels of K8s users get benefits. Lastly he advocates for people to contribute to Open Source. While I'm happy he's an advocate and I do like his config approach, I can't really tell what he's working on from the interview and there isn't much actionable in his config statements either.
BottomLine: Short interview with summary which is a bit too lightweight on DevOps and Kubernetes
==
URL reference: https://medium.com/@jp-gouin/gitops-at-scale-clusters-bootstrapping-f36695d4340d
DateReviewed: 2024-07-16
Description: This article dives into one approach to managing multiple cluster environments. We start with a quick background of what they want to cover; things like Git and spinning up a cluster, amoung others. They have a staging/qa/prod system, with 2 variants - prod and non-prod, and intend to deploy 2 apps - An Ingress controller and the cert manager. They explain they are going to have a base dir that contains the common for all envs, and a variants for common characteristics between envs, and an env folder with per-env instructions. They then lay out their plan for building the structure, and then show us a snip example of a file that fits in the system. They walk through a couple more snips with descriptions, and link to a full example and reference a course that explains further. Next up, they take us through deploying with GitOps and ArgoCD. They go through deploying two apps, with YAML files and a good deal of description and even a well thought warning. They call promotion what I'd say is upgrading, but the effect is the same - to change the version of the app that is running. In their case, they mention copying a newer spec file from staging to QA, and having the system pick up the drift between the YAML and the running config. It allows both manual and automatic sync, to bring the running config out of drift; the recommendation is to build trust using the manual method first. They then quickly go through the process of adding a new app, if you need to add new functionality. Finally they show how to add a new cluster -- which is simple in the system. Overall I like this approach, and it is a solid walkthough.
BottomLine: Great hands-on to build a complete dev to prod Kubernetes system
==
URL reference: https://medium.com/@kespineira/choosing-between-ip-and-instance-target-type-in-aws-load-balancers-for-eks-065fb9b548a7
DateReviewed: 2024-07-17
Description: This article focuses on using AWS Load balancers, often done in Kubernetes to expose services to the Internet, with an eye to the difference between an instance target type and an IP type of Load Balancer. We first start with an exam of ALB vs NLB, an ALB focused on HTTP types of traffic, and an NLB able to handle any type of IP traffic. We move onto target type, starting with the instance type, which routes traffic to the primary IP of the instance. It also touches on pros and cons, like this type working automatically with autoscaling and incorporating new intances automatically but introducing some extra overhead. Next we look at the IP type, which routes it to the end pods and not the node's instance IP. It also tackles the pros and cons, examples are this type is better suited for pods that are more ephemeral but this adds configuration overhead. We get a chart with some of the features compared by the two types, and some rules of thumb on which one to apply to our K8s clusters. We conclude with links to some resources.
BottomLine: Good overview of using various LoadBalancer target types in AWS to deliver traffic to Kubernetes
==
URL reference: https://www.aquasec.com/blog/kubernetes-exposed-exploiting-the-kubelet-api/
DateReviewed: 2024-07-18
Description: This article is going to talk about the Kubelet API and security fallout from bad configs that allow too generous of access. We start by diving right in, with the fact that the Kubelet API is powerful for users, but without good auth configured and exposed to the Internet can be very bad in many ways. Next up, we do a bit of dive into what the Kubelet API exactly is, which is to manage the pods and the containers run on each node of the cluster as an agent. They note that normally Kubelet only talks to the control plane, but for debugging direct comms can be enabled and they show the CLI to do so. They include a diagram of a K8s cluster with two nodes for illustration. Next, they show us some practical attacks that actually occured on the Internet against their own honeypot test setup. They outline 3 specific actor approaches, and then talk about general attacks in the wild, before wrapping up with 6 mitigation techniques. A very well done piece.
BottomLine: Great article on some of the ways Kubernetes is targeted for attacks
==
URL reference: https://aws.plainenglish.io/automate-dependency-updates-in-your-kubernetes-projects-f526cc24810a
DateReviewed: 2024-07-19
Description: The background for this is that projects often depend on other projects for their functionality, and it can be challenging to keep abreast of the latest versions of these dependencies. Some of the updates may be security related, so not keeping up to date may have security implications. Their approach is to automate updates, so let's take a look. Their process is simple - monitor for updates, fetch them, apply, then test, and finally create pull requests. For the monitoring piece, they use Dependabot, which looks through your projects dependency file and performs the versin check. They go through the process, which I actually like the approach of -- check, update, test and PR. The only weakness here lies in the strength of your tests. Next we get a hands on, where they explain how to set up Dependabot, including a sample config YAML and how to get things rolling. They even go through the pros and cons and mention some other tools. I think their approach is solid, as there is still a human element but a lot of the heavy lifting is done automatically.
BottomLine: Walkthrough on automating container dependency management
==
URL reference: https://medium.com/@imrancodes/understanding-deployment-strategies-pros-cons-and-implementation-46d416d171f1
DateReviewed: 2024-07-20
Description: Getting software out there is a crucial part of the development process, and this article will tackle a few different deployment strategies. They detail 6 approaches, each with an overview, pros and cons, and a short implementation snip. We start with recreate, which we tear down what we deployed before and spin up a whole new set of containers for our app. While not ideal for prod uptime, it's easy and might be perfect for hobby or niche services that don't need 24/7 availability. Next up is rolling update, where a new container spins up as one old container spins down, giving you full uptime. While more complex, it does offer easy rollback if things aren't working. We then move on to Blue/Green, where you have a live (blue) and a test (green), and switch over when the test is deemed good. Demands more resources, but seemless and instant switch and rollback ability. Next is Canary, where a small subset of users are directed to the new version while the rest stay at the current version. They then go through A/B testing, which is similar to canary but usually focused on feedback for specific features. Next we look at shadowing, where the traffic is mirrored and the user gets the production response where the new version is compared to. 
BottomLine: Overall a good walk through of different deployment appraches
==
URL reference: https://baykara.medium.com/part-1-how-kube-scheduler-works-in-kubernetes-be66118679c5
DateReviewed: 2024-07-21
Description: One of the important parts in Kubernetes orchestrating containers is kube-scheduler, and this article looks into that. They explain that K8s is a series of events triggerd or generated by controllers. They further mention that most things in K8s are pluggable, and the scheduler is too, allowing it to be extensible and maintainable. We start with a diagram, we dive into the architecture. Queues are the building blocks of the scheduler, and we start with one called the ActiveQ, which schedules pods immediately. They talk a bit about constraints a pod may have, like a need for a certain amount of memory or a specific volume, and how these are handled through two other queues. We then go through a pod creation, specifically the two cycles - scheduling and binding. In scheduling, the pods go through a number of steps to figure out where to run it -- they outline 8 steps. The bind phase preps and actually runs the pod, and then cleans up when it finishes running. We then do a quick runthrough of how this works.
BottomLine: Actually a great intro to how kube-scheduler works in Kubernetes
==
URL reference: https://vulmon.com/vulnerabilitydetails?qid=CVE-2024-5321
DateReviewed: 2024-07-22
Description: This is a Kubernetes vulnerability disclosure. The issue that is identified is that a user can read the logs, and someone with NT AUTHORITY can alter the logs. The reading might not be a serious issue, but having anyone on a node with a certain authority being able to alter the logs can mean that an attacker might be able to manipulate the logs. These issues are classified as medium severity.
BottomLine: Disclosure of a Windows specific security vulnerability in Kubernetes
==
URL reference: https://www.wiz.io/blog/sapwned-sap-ai-vulnerabilities-ai-security
DateReviewed: 2024-07-23
Description: This article postulates that AI may have an isolation issue, where by running various tennant AI models and apps together, it is akin to executing arbitrary code. For this iteration of their test, they focused on SAP AI core. They start by stating that the AI Core product integrates with other SAP cloud offerings, including HANA and other sensitive info. They wonder since such access might use secrets to gain access, if it is possible to recovery them through a constructed attack. They include a diagram and what they were able to expose, along with the details of the attack. Now, the attack is actually NOT AI specific -- they start with a simple restricted pod and look for interesting bits and build from there. They find a weakness that allows them to access the security sidecar as the security user, which allows them better network access. They found a logging process which they were able to get AWS keys from. They find a few further issues, including access to the image repository which allows them to write new containers to be executed, effectively giving them control of the cluster. Now, I like this walkthrough, and the premise is that access is because of AI, but these are config issues and not model or training parts being leveraged.
BottomLine: Good walkthrough of lateral security movement in Kubernetes
==
URL reference: https://medium.com/@lukwagoasuman236/security-kubernetes-clusters-and-certificates-take-2-59dd7b5a87b5
DateReviewed: 2024-07-24
Description: This is an overview of how Kubernetes clusters operate and how certificates keep the API secure. We start with the notion that all comms are expected to be done securely, and this means using certificates to facilitate it. Then they talk about Kube-apiserver, which is a server to external users and some cluster components, but is a client of etcd and kubelet. They mention etcd's role of storing cluster data and serving answers, and kubelet which allows the API to talk to worker nodes. They explain the concept of a user, the purpose of the scheduler, and where kube-controller-manager and kube-proxy fit. We now have the basic pieces, and next up they go through a complete walkthrough of generating and updating certs for various bits. Next up is a fair lengthed section on admission controllers and admission controller webhooks, which let you control what type of containers are run and gives policies on how they should run. Another good length section is on networks and network policy in Kubernetes. Okay, my take - these are well introduced and covered competently but in a basic way. Excellent for someone new to K8s, but you'd probably want more if you are a long time user.
BottomLine: Good intro to Kubernetes security with a focus on Certs and the API
==
URL reference: https://medium.com/@kedarnath93/what-is-gateway-api-in-kubernetes-and-how-does-it-differ-from-ingress-api-aa0404d7fc09
DateReviewed: 2024-07-25
Description: This article will highlight Gateway API, and how it differs from Ingress. First, we start with a brief re-cap of what Ingress is, which is basically HTTP and HTTPS routing for services. On the other hand, Gateway API allows almost arbitrary access to TCP and UDP services, allowing the range of services to be much more flexible. As Gateway is focused lower in the stack, it requires more flexible approaches and less assumptions about the underlying traffic. Part of the design of the Gateway API is in modularity and flexibility. Now we dive into how this works, starting with GatewayClass, which is a cluster level resource and has one Gateway that handles it. A Gateway is the piece that handles the traffic from the network to the back-end service, and a specific Gateway can be responsible for a few GatewayClasses. Routes are the protocol specific rules to map Gateways to specific services, and have a number of these included in Kubernetes. They do mention that there may be a single Route and Gateway in a service, or a Gateway can connect to multiple Routes, or multiple Gateways can connect to a single Route as needed. At this point, they get into examples, showing how things like namespace restrictions, Route type and port restrictions, traffic routing based on path or headers, redirects, traffic splitting and request mirroring.
My opinion is this seems like very interesting bits, and I'd love to play a bit with them soon.
BottomLine: Intro to Gateway API for Kubernetes and how it differs from Ingress
==
URL reference: https://medium.com/@innablr.au/kubernetes-pod-security-using-podsecuritystandards-c9faef4af073
DateReviewed: 2024-07-26
Description: Wait, it seems Kubernetes is not secure out of the box -- who knew? So this article is going to focus on user and process privs, or what can be done from inside your pod. They start by mentioning the older PodSecurityPolicy, which was the older way to control what a pod might do. They mention it can be confusing and hard to maintain, but might be already deployed. We then hone in on PodSecurityStandard, the newer way to tackle pod permissions. They start with a diagram, and explain some of the basics of how it works. They then go through the why, with emphasis on how it improves things over the previous PSP. We then get into a little demo, going through the CLI and showing what happens when we have various levels of enforcement. Actually a decent into.
BottomLine: Good intro to the newer PodSecurityStandard for Kubernetes
==
URL reference: https://medium.com/@sheetal.singhh241/kubernetes-stronghold-fortifying-your-clusters-with-cutting-edge-security-7435315bfc9c
DateReviewed: 2024-07-27
Description: Typically getting started security articles are stiff and boring, but this one starts light and witty -- even dropping a ref to Spiderman. We start with the basics in Kubernetes security wise, and this is RBAC. Simple one sentence explainations, a couple example YAMLs Role and RoleBinding to set up a read-only user. Next up is networks, and that is segmentation. They take us through setting up Network policies, again with a YAML example to allow same namespace ingress between pods. They show us Istio and then a YAML to set up mutal TLS for all services in a namespace. Moving on they set up monitoring with Grafana and Prometheus and give us the CLI commands to get that done. Our last practical piece is secret managemnt - they show how to create one on the CLI, then how to use it in a YAML spec. Honestly, it's not very expansive but it's an excellent place to start your K8s security journey.
BottomLine: Very light but good intro to Kubernetes security
==
URL reference: https://www.learnitguide.net/2023/04/kubernetes-architecture-explained.html
DateReviewed: 2024-07-28
Description: This is an article on Kubernetes architecture from a high level, and it is part of a series. We start with the master and worker node construct, and then talk about pods, ReplicaSets, Services and Deployments. Each action has a small paragraph explaining things to tie things together. There is a second section where it goes through various K8s CLI commands. Overall a good start to the high-level architecture pieces.
BottomLine: Short but decent intro to Kubernetes Architecture`
==
URL reference: https://github.com/dguyhasnoname/k8s-cluster-checker
DateReviewed: 2024-07-29
Description: A repo for a tool that checks Kubernetes configs. It'll check through things like OS vers, K8s/Docker vers, Admission controllers, security context, health probes and much much more. Generates screen, CSV and JSON output. It thn goes through a tour of the scripts that you can use to check your K8s config. We then have a walkthrough of how to run, an arch diagram and a sample run.
BottomLine: Interesting tool to check your Kubernetes config
==
URL reference: https://blog.devops.dev/unleashing-the-power-of-keda-for-dynamic-scaling-in-kubernetes-e4fc4008c3a7
DateReviewed: 2024-07-30
Description: This article is about KEDA, which focuses on controlling resources on a more raw level than Kubernetes usually does. They start with an overview of how KEDA is event-driven autoscaling, and then dive into the details. They talk about event sources, metrics, CPU and memory utilization as indicators of need to scale up or down. Next, a diagram, which I always love. They then go through some best practices, including clearly derfinded metrics an multiple event sources and the process of monitoring and optimizing based on observations. They go through a quick real-life case study, and then the conclusion. It's high level, but actually a decent start into KEDA.
BottomLine: A high-level piece to motivate usage of KEDA in Kubernetes
==
URL reference: https://medium.com/@anshumaansingh10jan/implementing-a-comprehensive-devsecops-jenkins-pipeline-technical-deep-dive-d62bf0a63906
DateReviewed: 2024-07-31
Description: Building dev pipelines in the cloud is about increasing speed, but it is important that security keep pace. This article walks through such a construct. We start with what they call the DevSecOps paradigm - the idea that security has to be considered and integrated into each step of the development process. Next we dive into leveraging an actual CI/CD tool, Jenkins, to utilize this process. We start with prereqs and installation. They have 7 packages as plugins to add, each covering a different need or area. We next are presented a Jenkins config file, with a number of the above plugins as part of the process. We are then presented the outputs as some screenshots. And that is it. So, it started well and had some promising bits, and then seemed to run out of steam. There isn't any explaination of the screenshots, not even "this shows X". It would have been helpful to know what they intended, as some of them showed various bits of the process that seemed successful. Maybe helpful if you already know Jenkins well?
BottomLine: Good start but weak end to DevSecOps pipeline walkthrough on Kubernetes
==
