URL reference: https://www.infracloud.io/blogs/unlocking-kubernetes-power-with-rke-custer-metallb-rook-ceph/
DateReviewed: 2024-07-01
Description: Blog post on running Rancher Kubernetes engine on a bare metal setup. We start with a walk through what RKE (Rancher K8s engine) is, and how it differs, including better defaults, US Fed. Gov't standards compliance and continuous vulnerability monitoring. They talk also about it's lineage, where it gets simplicity as a K3s derivative, and how they have shifted away from relying on Docker. They note setting up a proper amount of memory and disk space is crucial or you can run into issues with your K8s master. They also note that MetalLB doesn't work with many cloud providers, as they don't support its networking requirements, but it will work with VirtualBox. Next we move onto installing RKE2, which has a laundry list of prereqs and then we have to fetch, install and enable the rke2 service. Next we start the service and confirm its running by checking the logs. We do a couple kubectl commands and ensure our K8s is up. Next, we set up the worker nodes, which includes many of the above steps with the additional step of using the master token to join the existing cluster. Next we install the load balancer and configure it, with a test service to ensure it is working. We then move on to setting up storage with Rook-Ceph. We go through pre-reqs, install, create a storage cluster then make it available. There are a lot of moving parts here, and I think think it would have been better to break it into a few parts; but it is a solid intro to the pieces.
BottomLine: Deploy Rancher Kubernetes to bare metal with LB and Storage
==
URL reference: https://www.redhat.com/en/blog/friday-five-june-28-2024?sc_cid=701f2000000tyBjAAI
DateReviewed: 2024-07-02
Description: This is just a stub for the bigger "State of Kubernetes security in 2024", which RedHat does every year. It does link to a brief blog overview, which is what I'll review here. It gives an executive summary, with three charts and a feeling of what the surveyed participants feel are the tough parts of Kubernetes security today. It's good because it's a real world survey, and not someone's feelings on what should be where. It's a good toe dip.
BottomLine: Overview of the annual State of Kubernetes security.
==
URL reference: https://github.com/bunkerity/bunkerweb
DateReviewed: 2024-07-03
Description: BunkerWeb is a WAF (web application firewall) that is open source. They start with a quick overview, then a seven part segment on why, with each point being about a paragraph. They then do a rundown of the various security features, and finally go through resources, which includes a demo version to test out, a cloud variant, a paid Pro version and consulting services, and a long list of community focused resources and documentation. Looks very interesting.
BottomLine: A new open source WAF with cloud ability called BunkerWeb
==
URL reference: https://medium.com/@davis.angwenyi/how-to-install-grafana-loki-using-helm-e6b1185e5c24
DateReviewed: 2024-07-04
Description: This article discusses installing a comprehensive logging tool called Loki. We start with a diagram and there are a couple in here, which is always a good sign. They start with going through three variations of logging, monolithic, simple scalable and microservice, with a broken out explaination for each variant. It then goes through how the log bits work, tracing down both the write and read path for logging. Now we get into the install - so we add the repo in a helm CLI command, and then drop the YAML configs to be able to run the software. They then go through the YAML in chunks, explaining the various bits and what they do, and some of the options that are available. Once our config is ready, we can actually run the install to get things set up. This is the backend; they also run through how to set up and connect the client to send the logs. Pretty decent setup walkthrough.
BottomLine: Walkthrough install and basic info on the Grafana Loki Kubernetes logging piece.
==
URL reference: https://medium.com/intel-tech/how-to-containerize-your-local-llm-436182cd179a
DateReviewed: 2024-07-05
Description: This article is about using an LLM through an API from a container. They start with a walk-through of how to interact with an LLM and a bit on they why side, and then pose reasonable questions like how would you do this through a web framework. They give a link to the code, and then tackle an important question - why in a container. They talk a bit about using the model from internal or external spots, as well as saying that storing the model external to the container image can save space and make deployments faster. Basically they argue for decoupling the model from the container, to allow each to be updated seperately. You still need to store the model, which can run 26GB. They do point out that by running the model locally, you can tune the output it produces more finely. They then lead us a bit through the front end setup, and how to make it useful to devs and end users. They walk us through the Python code they use, and then show us how to create the Docker image for the container we'll be deploying. Finally they show us how to upload the image to a Registry for deployment.
BottomLine: Decent walk through on setting up an LLM in a container
==
URL reference: https://medium.com/adidoescode/adidas-how-we-are-managing-a-container-platform-1-3-6ce24e75649
DateReviewed: 2024-07-06
Description: This article is about a global scale collection of Kubernetes clusters, so you can get some insights into them working at that scale. This is focused on the apparel company Adidas, which deploys to 5 global areas with between 5 and 30 clusters and one or two regions within each area. They did use code to manage their configs, but it wasn't a globally shared code but rather branches for each region as they have their own idiosyncracies. There were lots of moving parts in this, including maint windows, different configs managed through the CI/CD process, with up to 50 configs that it was managing. Very interesting background and interesting items to understand.
BottomLine: Peek into a global scale Kubernetes deployment over many clusters
==
URL reference: https://www.itsecuritynews.info/portainer-open-source-docker-and-kubernetes-management/
DateReviewed: 2024-07-07
Description: This is a blurb that points to an actual article, so I'll keep this short. Portainer sounds like an interesting software package, able to manage Docker, Swarm, K8s and ACI envs with a GUI. This stump of an article, however, is not.
BottomLine: Stub which is actually not very helpful on Portainer - skip
==
URL reference: https://itnext.io/run-your-kubernetes-cluster-on-bare-metal-with-cilium-cni-part-1-e88028800d90
DateReviewed: 2024-07-08
Description: This article will help set up an on-prem home lab Kubernetes setup. We dive right in, first with the requirements, and they start with how the network has to be configured. Next up they explain the hypervisor, and both of these elements are tackled in detail. They then lay the background of our eventual target, with a pretty diagram to tie it together. Next up is the install, and they explain the hardware we'll need to proceed, before running us through the CLI commands to get everything on track. They then set up CRI, and explain with a breif explanation what that does. They then show us how to set up on two architectures, one ARM and one x86_64, depending on what CPU you'll be using. We go through a wall of CLI text, which is actually getting K8s to install and run, both for the masters and for the workers, and we even check to make sure it works. Next up is getting Cillium working, and again there is a large wall of text. My thoughts? It's a LOT of config, and I think it would help if there was a bit more "we are doing this for this reason", so people understand. But it's good to support getting a homelab up.
BottomLine: Walkthrough of getting a Kubernetes homelab up with Cilium
==
URL reference: https://it-notes.dragas.net/2024/07/04/from-cloud-chaos-to-freebsd-efficiency/
DateReviewed: 2024-07-09
Description: This article is a dive into a Kubernetes deployment which was complex, and the journey to simplify things. The choice here was to move from a cloud solution to a physical colo. On this base, they used various tech like FreeBSD jails to achieve things like virtualization and isolation. They also used ZFS, which is akin to a Logical Volume Manager on Linux. They tell a story about a project that got deleted, but was restored in a few minutes from the ZFS backups he had built. There is a lessons learned section, but to me it is a bit vague -- our author talks about the previous setup and the current one, but there are no benchmarks or hard numbers, just talk of autoscaling and odd development choices. There is another story of a possible cryptominer which was causing load spikes on their old setup, due to an exploit. My take is that different projects have different needs, and while some teams could definately jump environments, many teams could not.
BottomLine: Interesting article on moving from the Kubernetes cloud to FreeBSD jails
==
URL reference: https://medium.com/@ujjwalsapkota005/exploring-kubernetes-services-clusterip-nodeport-externalname-and-loadbalancer-22553b33910f
DateReviewed: 2024-07-10
Description: This article explores different ways to allow traffic to get to your services through the network. They review ClusterIP, NodePort, ExternalName and LoadBalancer. There isn't too much background so lets dive in. There are a number of diagrams, even if the text doesn't really reference them. We start with ClusterIP, which they say is local to the cluster, and good for internal services. They do show a YAML file and both how to deploy and how to create a ClusterIP on the CLI. Next up is NodePort, which exposes a port on each nodes static ports, and allows explicit external traffic. This too has YAML and CLI bits. We move onto to ExternalName, which maps a service to a DNS name, which allows services to reach an external service. We get YAML for the config. Last up is LoadBalancer, which routes traffic to one or more pods. We get YAML and CLI for this one. They do a few examples of various configs using some of these resources. An interesting walkthrough, but lacks a bit of explanation.
BottomLine: Good basic cover of various mainly inbound Kubernetes constructs
==
URL reference: https://bughunters.google.com/blog/6669874749636608/securing-the-container-world-with-policies-acjs-and-ctrdac
DateReviewed: 2024-07-11
Description: This blog is the introduction of two new controllers to the Kubernetes ecosystem. They are acjs, the Admission Controller with JavaScript, and ctrdac, the Containerd Admission Controller. They start with a section on a recap of Admission controllers, explaining a bit about what they are and how they work in a few paragraphs. We also get a diagram of how they work inside and even outside of K8s clusters. We then have sections for each new controller, where they go through the details. Following that, they go through 3 examples so you can understand how they work. Actually a good intro to new controllers.
BottomLine: Introduction of 2 new Kubernetes admission controllers
==
URL reference: https://medium.com/@harmandiaz023/navigating-kubernetes-cost-management-challenges-the-ultimate-guide-53d018905529
DateReviewed: 2024-07-12
Description: We start with Kubernetes good, but costs not managed are bad; so lets dive in. We next talk about cost challenges in K8s, which include bad resource allocation and optimization, complex pricing, multi-cloud envs, governance and compliance, estimation and budgeting. Each of these items gets a paragraph or so of coverage, so we know what is being done wrong. Next we look at cost mgmt strategies, and these include optimizing resource allocation, choosing the right cloud provider, and cost monitoring and reporting. I've seen a few of these, and this one seems lighter than others, and doesn't really say anything different or new.
BottomLine: Rehash of previous pieces on Kubernetes cost management
==
URL reference: https://www.computerweekly.com/feature/Kubernetes-at-10-When-K8s-won-and-life-now-as-a-surly-teenager
DateReviewed: 2024-07-13
Description: With Kubernetes turning 10, it is now time to reflect a little, and in that context, the person interviewed here believes K8s is in the "surly teenager phase. They start with talking about the software landscape when K8s launched, which included Docker Swarm and Apaache Mesos. We dive a bit into the interviewees background, which is in distributed DBs by way of Cassandra, and Spark, which fit well with Mesos. He says his lightbulb moment of when K8s "won" was when an enterprise Mesos vendor announced K8s support. He does mention that K8s started with poor storage support, which caused DB people to avoid it. The catalyst here was the introduction of StatefulSet support, which allowed persistent storage without 3rd party extras. The addition of operators cemented K8s position as it allowed control over how services were added and removed. They then talk a bit about the proliferation of various operators, and the shakeout to get some core ones going. This, they explain, helped drive cloud native adoption. He says K8s is still growing and evolving, while it has a solid base to grow on. A decent piece and good retrospective on K8s.
BottomLine: Ten year of Kubernetes through the eyes of a distributed DB guy
==
URL reference: https://itnext.io/automate-kubernetes-with-shell-operator-1ae5b50408ae
DateReviewed: 2024-07-13
Description: We start by reflecting on developing a Kubernetes operator usually means knowing K8s and Go well. To turn that on it's head, a group developed the "shell operator", which allows use of scripting langs like Bash, Python and Perl. To understand what this is all about, we're going to follow the fictional goal of getting real-time pod monitoring working. First, we start with a diagram which shows what we'll end up with, and it looks pretty complex. We now start with a discription of how the shell operator works -- namely, by watching for specific events. When an event it is watching for happens, it then executes a script in response, and typical events it looks for are creation, deletion or updates of resources. Now that we know the overall mechanism, next up is using the features, so first we go through them. First up is event handling, and this is how the operator finds out about things. Next is script execution, and this is how the operator responds to events, which can be written in a variety of languages. It runs scripts out of the hooks directory, which ensures flexibility; and you don't have to understand all of the underpinings to automate K8s. In the monitor, they use Terraform with a local Kind cluster along with crossplane to message Slack. We then see an example bash script to notify Slack on pod creation. We're then taken through setting up a Docker image with the shell operator, and then building and pushing this image to a an image repository. They even go through the needed RBAC to get the permissions right as YAML. Next up is the YAML to deploy the shell operator, along with the CLI to deploy the pieces. Now we should see create and delete request show up in Slack. Overall, it's a good intro.
BottomLine: Using the Shell Operator to automate Kubernetes with Python or Bash
==
URL reference: https://github.blog/changelog/2024-06-25-artifact-attestations-is-generally-available/
DateReviewed: 2024-07-14
Description: This is a blog article on Artifact attestatations. The idea is to create a way to build a link between what authors build and the actual software you are using by using a signed authority which isn't able to be tampered with. We get two endorsements, and then an actual walkthrough of what is needed to activate this - in GitHub actions, you add a snip that references your code and pulls in the attest-build-provenance action, and then run it with a CLI command as part of your build process. The announcemnt also says they've released a controller that checks attestations, so you can see that you are using secured software.There is also a video to take you through the process. This is an interesting approach to security, esp. for items that are more than one dependency level deep.
BottomLine: Blog on using attestations in your builds on GitHub to secure your Kubernetes software
==
URL reference: https://securityboulevard.com/2024/07/securing-kubernetes-the-risks-of-unmanaged-machine-identities/
DateReviewed: 2024-07-14
Description: Containers and microservices change the way we build apps, but because of the big shift things like Kubernetes open up new attack surfaces. By spinning up (and down) containers on demand, this makes security more challenging. And apparently every pod could be a gateway for attackers, so each should be protected with the highest level of security. It seems existing tools are not well suited to the short-lived nature of some pods. I pause here, as this is overly hard -- some pods are long lived, and not every pod is a great security risk. We should be vigilant, but this take is too harsh. They now circle back to the basics, including the four C's, as a starting point. (Cloud, cluster, container and code). We then get into a bit about IAM and accounts, before them actually making some good points. They mention lack of visibility, no centralized root of trust, disconnected processes and team silos, ops bottlenecks and lack of control as issues, mainly for manual processes. This is a good observation, and there are a few paragraphs in each item to back things up. Our wrap paragraph is our ad bit, for AppViewX which helps with all of this.
BottomLine: A few late good points but some bad rattling in the opening mar this examination of Kubernetes security
==
URL reference: https://allthingsopen.org/articles/security-posture-and-standard-configurations-across-kubernetes-clusters
DateReviewed: 2024-07-15
Description: This is a short YouTube interview with a written summary on various DevOps pieces. The interview subject did a talk on Pepr, which enhances your security posture; but he doesn't explain that much further. He does talk a bit about configuration challenges, and strongly advocates for standardizing the approach no matter where your cluster is running, so all levels of K8s users get benefits. Lastly he advocates for people to contribute to Open Source. While I'm happy he's an advocate and I do like his config approach, I can't really tell what he's working on from the interview and there isn't much actionable in his config statements either.
BottomLine: Short interview with summary which is a bit too lightweight on DevOps and Kubernetes
==
