URL reference: https://www.infracloud.io/blogs/unlocking-kubernetes-power-with-rke-custer-metallb-rook-ceph/
DateReviewed: 2024-07-01
Description: Blog post on running Rancher Kubernetes engine on a bare metal setup. We start with a walk through what RKE (Rancher K8s engine) is, and how it differs, including better defaults, US Fed. Gov't standards compliance and continuous vulnerability monitoring. They talk also about it's lineage, where it gets simplicity as a K3s derivative, and how they have shifted away from relying on Docker. They note setting up a proper amount of memory and disk space is crucial or you can run into issues with your K8s master. They also note that MetalLB doesn't work with many cloud providers, as they don't support its networking requirements, but it will work with VirtualBox. Next we move onto installing RKE2, which has a laundry list of prereqs and then we have to fetch, install and enable the rke2 service. Next we start the service and confirm its running by checking the logs. We do a couple kubectl commands and ensure our K8s is up. Next, we set up the worker nodes, which includes many of the above steps with the additional step of using the master token to join the existing cluster. Next we install the load balancer and configure it, with a test service to ensure it is working. We then move on to setting up storage with Rook-Ceph. We go through pre-reqs, install, create a storage cluster then make it available. There are a lot of moving parts here, and I think think it would have been better to break it into a few parts; but it is a solid intro to the pieces.
BottomLine: Deploy Rancher Kubernetes to bare metal with LB and Storage
==
URL reference: https://www.redhat.com/en/blog/friday-five-june-28-2024?sc_cid=701f2000000tyBjAAI
DateReviewed: 2024-07-02
Description: This is just a stub for the bigger "State of Kubernetes security in 2024", which RedHat does every year. It does link to a brief blog overview, which is what I'll review here. It gives an executive summary, with three charts and a feeling of what the surveyed participants feel are the tough parts of Kubernetes security today. It's good because it's a real world survey, and not someone's feelings on what should be where. It's a good toe dip.
BottomLine: Overview of the annual State of Kubernetes security.
==
URL reference: https://github.com/bunkerity/bunkerweb
DateReviewed: 2024-07-03
Description: BunkerWeb is a WAF (web application firewall) that is open source. They start with a quick overview, then a seven part segment on why, with each point being about a paragraph. They then do a rundown of the various security features, and finally go through resources, which includes a demo version to test out, a cloud variant, a paid Pro version and consulting services, and a long list of community focused resources and documentation. Looks very interesting.
BottomLine: A new open source WAF with cloud ability called BunkerWeb
==
URL reference: https://medium.com/@davis.angwenyi/how-to-install-grafana-loki-using-helm-e6b1185e5c24
DateReviewed: 2024-07-04
Description: This article discusses installing a comprehensive logging tool called Loki. We start with a diagram and there are a couple in here, which is always a good sign. They start with going through three variations of logging, monolithic, simple scalable and microservice, with a broken out explaination for each variant. It then goes through how the log bits work, tracing down both the write and read path for logging. Now we get into the install - so we add the repo in a helm CLI command, and then drop the YAML configs to be able to run the software. They then go through the YAML in chunks, explaining the various bits and what they do, and some of the options that are available. Once our config is ready, we can actually run the install to get things set up. This is the backend; they also run through how to set up and connect the client to send the logs. Pretty decent setup walkthrough.
BottomLine: Walkthrough install and basic info on the Grafana Loki Kubernetes logging piece.
==
URL reference: https://medium.com/intel-tech/how-to-containerize-your-local-llm-436182cd179a
DateReviewed: 2024-07-05
Description: This article is about using an LLM through an API from a container. They start with a walk-through of how to interact with an LLM and a bit on they why side, and then pose reasonable questions like how would you do this through a web framework. They give a link to the code, and then tackle an important question - why in a container. They talk a bit about using the model from internal or external spots, as well as saying that storing the model external to the container image can save space and make deployments faster. Basically they argue for decoupling the model from the container, to allow each to be updated seperately. You still need to store the model, which can run 26GB. They do point out that by running the model locally, you can tune the output it produces more finely. They then lead us a bit through the front end setup, and how to make it useful to devs and end users. They walk us through the Python code they use, and then show us how to create the Docker image for the container we'll be deploying. Finally they show us how to upload the image to a Registry for deployment.
BottomLine: Decent walk through on setting up an LLM in a container
==
URL reference: https://medium.com/adidoescode/adidas-how-we-are-managing-a-container-platform-1-3-6ce24e75649
DateReviewed: 2024-07-06
Description: This article is about a global scale collection of Kubernetes clusters, so you can get some insights into them working at that scale. This is focused on the apparel company Adidas, which deploys to 5 global areas with between 5 and 30 clusters and one or two regions within each area. They did use code to manage their configs, but it wasn't a globally shared code but rather branches for each region as they have their own idiosyncracies. There were lots of moving parts in this, including maint windows, different configs managed through the CI/CD process, with up to 50 configs that it was managing. Very interesting background and interesting items to understand.
BottomLine: Peek into a global scale Kubernetes deployment over many clusters
==
URL reference: https://www.itsecuritynews.info/portainer-open-source-docker-and-kubernetes-management/
DateReviewed: 2024-07-07
Description: This is a blurb that points to an actual article, so I'll keep this short. Portainer sounds like an interesting software package, able to manage Docker, Swarm, K8s and ACI envs with a GUI. This stump of an article, however, is not.
BottomLine: Stub which is actually not very helpful on Portainer - skip
==
URL reference: https://itnext.io/run-your-kubernetes-cluster-on-bare-metal-with-cilium-cni-part-1-e88028800d90
DateReviewed: 2024-07-08
Description: This article will help set up an on-prem home lab Kubernetes setup. We dive right in, first with the requirements, and they start with how the network has to be configured. Next up they explain the hypervisor, and both of these elements are tackled in detail. They then lay the background of our eventual target, with a pretty diagram to tie it together. Next up is the install, and they explain the hardware we'll need to proceed, before running us through the CLI commands to get everything on track. They then set up CRI, and explain with a breif explanation what that does. They then show us how to set up on two architectures, one ARM and one x86_64, depending on what CPU you'll be using. We go through a wall of CLI text, which is actually getting K8s to install and run, both for the masters and for the workers, and we even check to make sure it works. Next up is getting Cillium working, and again there is a large wall of text. My thoughts? It's a LOT of config, and I think it would help if there was a bit more "we are doing this for this reason", so people understand. But it's good to support getting a homelab up.
BottomLine: Walkthrough of getting a Kubernetes homelab up with Cilium
==
URL reference: https://it-notes.dragas.net/2024/07/04/from-cloud-chaos-to-freebsd-efficiency/
DateReviewed: 2024-07-09
Description: This article is a dive into a Kubernetes deployment which was complex, and the journey to simplify things. The choice here was to move from a cloud solution to a physical colo. On this base, they used various tech like FreeBSD jails to achieve things like virtualization and isolation. They also used ZFS, which is akin to a Logical Volume Manager on Linux. They tell a story about a project that got deleted, but was restored in a few minutes from the ZFS backups he had built. There is a lessons learned section, but to me it is a bit vague -- our author talks about the previous setup and the current one, but there are no benchmarks or hard numbers, just talk of autoscaling and odd development choices. There is another story of a possible cryptominer which was causing load spikes on their old setup, due to an exploit. My take is that different projects have different needs, and while some teams could definately jump environments, many teams could not.
BottomLine: Interesting article on moving from the Kubernetes cloud to FreeBSD jails
==
URL reference: https://medium.com/@ujjwalsapkota005/exploring-kubernetes-services-clusterip-nodeport-externalname-and-loadbalancer-22553b33910f
DateReviewed: 2024-07-10
Description: This article explores different ways to allow traffic to get to your services through the network. They review ClusterIP, NodePort, ExternalName and LoadBalancer. There isn't too much background so lets dive in. There are a number of diagrams, even if the text doesn't really reference them. We start with ClusterIP, which they say is local to the cluster, and good for internal services. They do show a YAML file and both how to deploy and how to create a ClusterIP on the CLI. Next up is NodePort, which exposes a port on each nodes static ports, and allows explicit external traffic. This too has YAML and CLI bits. We move onto to ExternalName, which maps a service to a DNS name, which allows services to reach an external service. We get YAML for the config. Last up is LoadBalancer, which routes traffic to one or more pods. We get YAML and CLI for this one. They do a few examples of various configs using some of these resources. An interesting walkthrough, but lacks a bit of explanation.
BottomLine: Good basic cover of various mainly inbound Kubernetes constructs
==
URL reference: https://bughunters.google.com/blog/6669874749636608/securing-the-container-world-with-policies-acjs-and-ctrdac
DateReviewed: 2024-07-11
Description: This blog is the introduction of two new controllers to the Kubernetes ecosystem. They are acjs, the Admission Controller with JavaScript, and ctrdac, the Containerd Admission Controller. They start with a section on a recap of Admission controllers, explaining a bit about what they are and how they work in a few paragraphs. We also get a diagram of how they work inside and even outside of K8s clusters. We then have sections for each new controller, where they go through the details. Following that, they go through 3 examples so you can understand how they work. Actually a good intro to new controllers.
BottomLine: Introduction of 2 new Kubernetes admission controllers
==
URL reference: https://medium.com/@harmandiaz023/navigating-kubernetes-cost-management-challenges-the-ultimate-guide-53d018905529
DateReviewed: 2024-07-12
Description: We start with Kubernetes good, but costs not managed are bad; so lets dive in. We next talk about cost challenges in K8s, which include bad resource allocation and optimization, complex pricing, multi-cloud envs, governance and compliance, estimation and budgeting. Each of these items gets a paragraph or so of coverage, so we know what is being done wrong. Next we look at cost mgmt strategies, and these include optimizing resource allocation, choosing the right cloud provider, and cost monitoring and reporting. I've seen a few of these, and this one seems lighter than others, and doesn't really say anything different or new.
BottomLine: Rehash of previous pieces on Kubernetes cost management
==
