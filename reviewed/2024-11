URL reference: https://medium.com/@josielol/unleashing-the-t-rex-how-containers-sometimes-fail-to-contain-1710b6cc7990
DateReviewed: 2024-11-01
Description: Our article here is going to touch on how containers sometimes fail to contain - and they start with an analogy to Jurrasic Park, and how the dinos were not contained. We dive in starting with container basics, going though how it's a single bundle running on an OS, which is mostly isolated. They point out that at a low level, a container is just a child on the host system. We get a neat diagram illustrating containers/namespaces and the Linux features that help out. The container runtime are the gates, allowing containers to start, do things, and stop. But they say, gates sometimes fail. We get another diagram, with arrows and a label of container escapes, so let's see what we're going to delve into. They talke about permission issues with mounts, allowing unintended file access; or a buffer overflow, allowing access to shared memory. Misconfigs can even allow CLI commands on the host from the container, and lastly, priv escalation that results in root access, allowing the attacker carte blanche. Next up they are going to do a demo with eBPF. They say that clever code can get around the verifier and JIT compiler, allowing execution of unsafe bytecode. They go into a bit of detail on how the verifier works, ensuring there are no unreachable instructions and simulating each control branch. They point to a few actual instances of CVEs involving the verifier, and are going to focus on a recent one. The walkthrough is quite simple - we build a user container, include the nasty code in the image, then run the binary and are root when done. Pretty cool. They go into mitigations with supply chains and SBOMs, but basically you have to be vigilant. Excellent article.
BottomLine: Awesome article showing a straightforward container escape in Kubernetes
==
URL reference: https://achievers.engineering/load-testing-kubernetes-resolving-bottlenecks-and-improving-performance-part-2-c4f08102f105
DateReviewed: 2024-11-01
Description: This article is a followup on a previous case where they did some performance tuning. They take that as a baseline and attempt to get even more throughput here. We start by looking at metrics like throughput, errors, latency and scalability. They assign goals, like doubling the baseline throughput, as where they want to go. They mention that as Kubernetes scales the cluster, that current strategy doesn't take full advantage, as already connected clients stay with the same pods, impeding proper load balancing. They show this in a graph. They say using Istio solves this, as it has proxies to help loadshare better, and so they move to this. They show the YAML needed, and an after graph, showing a much more even usage among the clients. They talk a bit about errors and latency, and it's important to note these are config only changes -- none of this needs any code change so far. Next they show how by doing tiny tweaks you can get some exta performance out of the system. They go through a few more esoteric tweaks, before circling back and noting they had achived all of the goals they had put out. Interesing but highly technical piece.
BottomLine: Low level performance tweaks to get more out of your Kubernets cluster
==
