URL reference: https://dev.to/krestomatio/introducing-lms-moodle-operator-251l
DateReviewed: 2024-05-01
Description: This article is about deploying Moodle on Kubernetes, so let's get started. The approach of our authors is to leverage an Operator, which they say works as a meta-operator to handle the entire stack, including Postgres, and other apps and even Moodle itself. They then try to sell us, explaining this gives us automatic deployment, meta-operator arch, full stack management and CRDs, with a sentence of explaination. We are then taken through the why of this approach, but honestly they still haven't explained why Operators are the way to go or anything about how they work by this point. They then explain how to get started, which is a quick three step hand wave at the process; to their credit, they do link to the docs which give a better runthrough. They then have a blurb for their managed service, where they run all of this for you. Ok, so my impression is one of disappointment -- I was interested in how Moodle can be married with K8s, and there isn't much here. If you were a Moodle person this means basically nothing, as they only mention Moodle in passing. Even as someone who has worked with K8s before, they don't explain why THIS is the way to run Moodle on K8s. It MIGT be a great way, but this article doesn't sell it like that.
BottomLine: Article on using Moodle on Kubernetes which isn't up to snuff
==
URL reference: https://medium.com/@setiadi.galih65/cloud-logging-on-kubernetes-engine-18627f206705
DateReviewed: 2024-05-02
Description: This article is going to talk about cloud logging, which is an interesting topic. They talk first about gathering all logs in one place, which is a good strategy. They are going to take us through a lab, using Terraform to deploy a log generating app so we can fiddle with cloud logs. Next, we have a diagram, and explain we'll use a K8s GKE cluster to generate logs which will be fed into stackdriver and then the cloud log we want to see. They set some configs, and then have the whole TF file - not in a small download box, but written out in full; it has comments, but no explaination. No CLI commands, just screenshots of TF running. We then visit a dashboard to confirm the cluster is running. We then visit the apps web page several times to generate log entries. We then take a peek at the web UI for viewing log entries. They talk about feeding logs into cloud storage and BigQuery for other analysis. So, I'm disappointed; the article is loose and sloppy - it handwaves at creation and ignores explaining what to do or why. It also seems to think logs are for debugging in the moment, and not for looking back -- it does cover long-term persistent storage, but more as a bonus then as a central theme.
BottomLine: Interesting topic of cloud logging but not well covered here
==
URL reference: https://swindonlink.com/news/kubernetes-security/
DateReviewed: 2024-05-03
Description: Kubernetes security is supposed to be part of the full lifecycle of an app, according to this article. Oddly, they claim the three phases are development, deployment and runtime? They mention malware and other risks in the opening too, so let's see what this is about. From that lofty height we drop right into node security, without any context, so okay. They do explain what nodes are and handwave for a paragraph about security updates, scans and pen tests, and using good auth. Next is API security, and they mention RBAC and review of server logs. Moving on, next is pod security, which they briefly explain before mentioning you must use a security context without explaining that. They also advocate for up to date images and other pod-focused security measures. We then move on to Data security, where they do mention encryption for data at rest and data in flight, backup and DR, and handwave at monitor and audit of data. After having a paragraph an area format, we pivot to a more loose format for what they call Enterprise security control, which is headings to things they covered earlier with different names. We move back to paragraphs under a section called common security threats, with subsections called pod to pod networking, config management, runtime risk, container image management, and lack of visibility. So, the title and the early bits felt promising, but the article starts as pablum and never really circles back to DevOps at all. I'd say it feels like they have all the ingredients here, but they lost the recipie so just smushed everything together.
BottomLine: Messy and shallow cover of Kubernetes security with no DevOps covered
==
URL reference: https://medium.com/@psakets/add-custom-headers-to-outgoing-request-from-applications-deployed-on-kubernetes-64d210c46bd9
DateReviewed: 2024-05-04
Description: We start with Kubernetes being the defacto place to deploy microservices, but no insight into why we'd inject custom headers, which is the focus of this article. We dive in with a walkthrough of how pods work, and how services leverage inter-pod comms to reach a service. We now do a walkthrough of service mesh, explaining a few of the things it does, and moving to use a specific one, in this case, Istio. We get an enhanced diagram showing how Istio handles straffic and how, in contrast to the pure service approach. We finally are at our payoff - header injection. We're told that Istio uses a component called Envoy, and this allows us to add filters to do things like logging, auth and even request modification, and can use multiple filters if needed. They explain we need a CRD to override a function, and the show us the YAML needed to get the override in place. The filter itself is a snip of Lua code, and is part of the YAML file. They also show us a longer function which is more complex and figures out if the target host is one of the ones we should be sending to. They then show how to add logging and then how to view the logs, to see what is going on. 
D
BottomLine: Short but interesting look at using service mesh to inject headers into Kubernetes services
==
URL reference: https://ostridelabs.com/kubernetes-security-10-basic-principles/
DateReviewed: 2024-05-05
Description: Kubernetes great, but wow - security issues - who would have thought? Focus of this article is 10 principles for securing your clusters. So amazingly, each of the ten areas gets a 3 to 5 paragraph discussion, which is way longer than a typical piece. I'll highlight the areas they go over - RBAC, strong authentication & authorization, network traffic, regular updates, good secret security, log management, using container sec best practice, isolate workloads, segment networks and finally a comprehensive policy management system. So, wow. It's not a hands on, but it's a lot of discussion and some really great points. A good long read with a good amount of high level detail.
BottomLine: Good read of 10 areas to cover in securing your Kubernetes cluster 
==
URL reference: https://medium.com/@neeraj_kumar_/from-code-to-cloud-deploying-your-node-js-app-on-aks-bb2b6d1711dc
DateReviewed: 2024-05-06
Description: This article will walk through how to set up a simple Node.js app on Azure Kubernetes. They assume you've already installed the basic Node.js package, but then they do walk us through how to create a new project via the CLI. We then install the express framework through npm. They then show us a minimal Node.js app, with a few paths and some logging. We're then shown the CLI command to run our app, and when we navigate to localhost on port 3000 we can interact with our app. Next, we have to package our app, so they walk us through preparing a Dockerfile which will do just that. We take that file, drop it in the root of our Node.js project, and then run a CLI docker build command. With our built image, we now need to store it somewhere so we can snag it from Kubernetes. Since we're trying to use Azure, it makes sense to use the Azure registry ACR. We need to use the web UI to create a registry, and they help us navigate through here. Once everything is set up, we want to push our previously built image to the ACR. They walk us through logging in, tagging, and finally pushing our image to the ACR. They show us how to check in the web UI that our image is uploaded. They then walk us through setting up a cluster in Azure. We then dip our toe into Kubernetes namespaces. Now we finally can deploy our app -- and we have a YAML file which we'll use to create a pod with our ACR image. Once deployed, they then show how to verify it, before showing how we can connect to our service from the Internet. So, overall, pretty cool. One annoyance is it seems that this was origianlly a series of articles, which they pulled all together, but without reworking; and each small section has a "conclusion", where it would be more apropos to have a "section summary".
BottomLine: Interesting hands on deploying a basic Node.js app in Azure Kubernetes
==
URL reference: https://www.puzzle.ch/de/blog/articles/2024/05/03/a-shift-left-philosophy-for-kubernetes-manifest-validation
DateReviewed: 2024-05-07
Description: This article explores "shift left", or the idea that security should be done earlier in the development pipeline. We start be looking at what they call the current state - this varies from using Helm to deploy and contrasts with a CD tool like Argo CD - but no where in this toolset is security considered. They note the introduction of shift-left, and what it means which boils down to what they call validation. They approach validation using the Rendered Manifests pattern, which gives them a simple way to explore what features a deployment has. They get practical with a tool called kubectl-validate, and we take it for a tour. Having Go installed is a prereq, and they show the Go command to install the validate, then show the CLI to actually do the validation. While it lets you validate Kubernetes built in types like services and deployments, the real interesting bit is being able to dig into CRDs. They then take us on a tour of an actual validation, showing us the commands we have to use and the YAML the resources use that we are going to validate. It then shows an issue that kubectl-validate catches. Finally, they do mention that there are things that validation doesn't support, but that it is in the pipeline.
BottomLine: Good introduction and hands on to shift-left security in Kubernetes
==
URL reference: https://nidhiashtikar.medium.com/kubernetes-ingress-host-based-ingress-and-path-based-ingress-4d82d1eedb14
DateReviewed: 2024-05-08
Description: This article is about Ingress, and we jump right in, with an explaination of the various types of routing available to inbound traffic. There are six common types, including host-based, path-based, simple fanout, virtual hosting, TLS termination and redirects & rewrites. This article focuses on host and path baed ingress. Host based is simple -- based on the host in the HTTP request, send the request on to the right pod(s) for a response. So each app needs it's own DNS entry, and our author shows us how to set this up with some YAML. Path based is also fairly simple, in that it looks for the request path to figure out how to route the request, and it sends it on to the right pod. So requests to /app1 go to pod app1, and requests to /app2 to to pod app2. Again, we're shown how to do this in YAML. It would have been nice to understand a bit of why we might do these things, but this is a good starting point.
BottomLine: Takes us through Kubernetes Ingress with Host and Path based types
==
URL reference: https://blog.stackademic.com/building-a-real-time-cricket-score-tracker-with-flask-and-deploying-it-to-kubernetes-with-jenkins-a87bd7999c92
DateReviewed: 2024-05-09
Description: This article is a walkthrough with a focus on deploying a Flask app with Jenkins on Docker & Kubernetes. The app is a Cricket score tracker which will build and deploy, and uses the Cricbuzz API. We start with an (unlabelled) diagram and an overview of how things will work. Use Flash to build the web app, Docker to build the image, deploy to K8s and use Jenkins to automate the CI/CD. They start with creating a new repo in Git, and then how to sign up for free to access the score API. Next we walk through the Python that will be run with Flask that will do the logic, and then create the HTML templates using jinja. They test the Flask app to validate everything is working. Then then walk through creating a Docker file and requirements.txt, to pull everything needed in; and show how to build the image and then run and test the image. We are then quickly walked throught the YAML and CLI to deploy to Kubernetes. Lastly, they set up Jenkins to be attached to our Repo, and configure it to run on a code commit to the repo. They show the pipeline file needed to run the code, and finally they show how to push code changes to the remote repo. Overall a decent walkthrough; but could explain some of the steps and would be nice to include some thoughts on debugging.
BottomLine: Decent walkthrough of a Flask app with Jenkins deployed to Kubernetes
==
URL reference: https://weng-albert.medium.com/updating-kubernetes-certificates-easy-peasy-en-139fc07f26c8
DateReviewed: 2024-05-10
Description: So, this article will look at TLS certificates used inside the Kubernetes cluster. We start off going through the defaults -- which in a somewhat recent version of K8s that certificates have a one year expiration. They further note that if we keep K8s up to date, certs are updated as part of the upgrade process. They then show the various certs that a typical system has, with other meta info like kind of cert. Next, we're going to walk through inspecting for expiry and backing up current certs. We start with some CLI commands to figure out expiry, along with screen caps showing some example returns. We then are shown how to back up the certs (to an alternate directory). Next up, they walk us through the commands to renew certs and check the validity, again with example screencaps, showing the new expiry dates. They then re-gen the configs on the master (without explaining why), and we check this has happened. Overall decent, but there is no indication on how to restore the backup we've created.
BottomLine: Walkthrough of checking and renewing certificates on Kubernetes
==
URL reference: https://community.hpe.com/t5/the-cloud-experience-everywhere/cloud-native-databases-on-kubernetes-where-are-we-today/ba-p/7213250
DateReviewed: 2024-05-11
Description: Our piece starts with the idea that the guidance at one time was not to run Databases on Kubernetes -- but is that still true today? We do a bit of history, starting with K8s being ideal for stateless workloads, and mentioning persistent volumes appearing in 2018. The authors say the game changer really happened in 2020, when CRDs hit the scene. We now enter a sidebar, which talks about OpenSource DBs with a focus on PostgreSQL. They feel this is the best DB, including DR features. They touch on CloudnativePG, a version of PostgreSQL that is adapted to run in K8s clusters. They get into the fine details of running a DB server, including replicas, logs, and even fast IOPS disks. Overall an interesting exploration.
BottomLine: Article examining whether you should run DBs in Kubernetes from an enterprise perspective
==
URL reference: https://thenewstack.io/egress-gateway-assign-stable-ips-to-traffic-leaving-k8s-clusters/
DateReviewed: 2024-05-12
Description: We dive right in to the point - in on-prem, we have stable IPs, but once we move to Cloud Native we don't always have control over IPs. While ideally we could do away with this requirement, there will be some need while things transition -- so how can we address this need? This is what the article dives in to. They list some scenarios - like what workload initiated this traffic, and how they access external third parties. Next we look at how Kubernetes handles this by default -- which is to give the outbound traffic the IP address of the pod that sent it. But pods come and go, and we can't rely on this for setting up firewall rules. Our authors propose an Egress gateway to solve this; basically they will NAT the traffic from a pod workload to a specific IP that the remote firewall will be happy with. They do mention that such gateways can be highly available, as once they have the config any of them can do the required NAT. Now, the authors argue this enhances security; but I'd argue that it complicates the network for legacy reasons, and doesn't really help with security. It's an interesting approach and has valid use cases, but I think using CERTs would be a more secure and robust approach, but understand that causes breaking changes.
BottomLine: Interesting approach for using fixed IPs for Kubernetes outbound egress
==
URL reference: https://www.tigera.io/blog/deep-dive/calico-vpp-empowering-high-performance-kubernetes-networking-with-userspace-packet-processing/
DateReviewed: 2024-05-13
Description: Networking in Kubernetes is important, and this Calico VPP thing allows neat stuff it seems, but they don't really explain up front what we're getting; and this is kind of a product anouncement, so that isn't good. They do include diagrams and explainations, so that is good. It does expose shared memory interfaces, by which I believe they mean that you have more access to the network packets for things like debugging akin to promiscuous mode. They also expose advanced load balancing, which I believe allows your clients to talk to the LB and either direct requests or move them to the right server. It also supports a userspace hoststack, which allows things like TLS and QUIC at the pod level; they also mention TCP and UDP, but don't containers already have them? They do chime in with 10Gbps networking ability and what seems to be a kind of K8s specific VLAN of some kind, which is cool. Honestly, it seems like it COULD be a cool tech, except it's not super clear what it is or how it works.
BottomLine: Unclear intro on how Calico VPP can help Kubernetes networking
==
URL reference: https://securityboulevard.com/2024/05/kubernetes-rbac-essentials-how-to-enable-and-manage-access/
DateReviewed: 2024-05-14
Description: We start with the possible need to add restrictions to certain K8s deployed apps, and this article will focus on doing that with RBAC. We dive in with a what is RBAC paragraph, and then get into Roles and RoleBindings. Next up is a paragraph on permissions and verbs, then a quick runthrough on user and service accounts. They then explain that this is all configured through K8s dynamic API for auth. Next, we go through enabling RBAC. Except, their thought of what "step by step" differs from mine; they describe the process, where I assume this means showing the commmands. They do mention checking status on AKS, which I assume is where their home base is. They go through defining roles and permissions, and while they have good advice and appraoch, again, not step by step. They talk next about RoleBindings and ClusterRoleBindings, explaining what they do and the difference between them. Ah, THIS time, they DO give us a YAML file and the CLI command; but confusingly, they have a code snip and include the YAML in the text. They then speak a bit to the drawbacks, before getting into their product pitch, which of course does whatever vanilla RBAC doesn't. While not perfect, there are definately good parts; it's just not great for someone brand new to RBAC.
BottomLine: Decent Kubernetes RBAC which is more high level as it lacks some of the needed hands on
==
URL reference: https://medium.com/@tamerbenhassan/falco-the-watchdog-of-kubernetes-security-2f6e0c2032ca
DateReviewed: 2024-05-15
Description: Kubernetes good, K8s security bad. So, this article will look at something to improve that, namely Falco. We start with a diagram, which is cool, and shows the flow of information from events through rules to alerts. We do a paragraph on what Falco is, being an open-source real time detection tool. Then we highlight what makes Falco tick at a high level - rules, configurable, monitors system calls and is easy to integrate. We then walk through a bit of the what - Intrusion and anomoly detection, audit trail and compliance, and each gets a few sentences of treatment. Oh, AND a hands on ... we start with how to intall, get a snipped of a YAML file to configure it with, and finally have a way to let a person know something is amiss. Actually a good toe dip into the tool.
BottomLine: Intro and walkthrough of open source security tool Falco
==
URL reference: https://blog.min.io/kubernetes-v1-30-enhancements/
DateReviewed: 2024-05-16
Description: This is a blog article about how MinIO is affected by the latest K8s v1.30 release. They talk about the new PodSecurity admission and how even though MinIO runs in user space without root perms, the new security approach is helpful. Another feature they discuss is tightening of volume modes for PersistentVolume snapshots. They talk about AppArmor being deemed stable, what it is and how to leverage it to enhance protection with MinIO. They include some YAML snips for a couple of the previous scenarios. They give a paragraph each to network improvements and contextual logging, before stronly recommending upgrading to the latest Kubernetes.
BottomLine: Decent discussion of new Kubernetes security bits in v1.30 which MinIO can benefit from
==
URL reference: https://securityboulevard.com/2024/05/simplify-certificate-lifecycle-management-and-build-security-into-openshift-kubernetes-engine-with-appviewx-kube/
DateReviewed: 2024-05-17
Description: Kubernetes is a good orchestration platform, and OpenShift builds upon it as a container app platform. This article will focus on an OpenShift extension. OpenShift provides things like monitoring and CI/CD pipelines, along with security and multi cloud support. One weakness though is lack of TLS management. They point out that a complex K8s deployment can have dozens or even of hundreds of certs, so managing these effectively is important. They then present 5 scenarios you may need to use a TLS cert, based on where the communications is aimeda at. We then go through a laundry list of cert management issues, and then they introduce the solution - AppViewX KUBE+, a name for the ages. It does discovery of certs, automates the process and allows policy to drive it. And, that's it; no tour, no further details, just magic. Always what you want from a vendor solution. 
BottomLine: Vendor piece with focus on TLS mgmt for Kubernetes but without much detail on the mitigation
==
URL reference: https://blog.devgenius.io/harnessing-kubernetes-metrics-for-technical-optimization-8a9b488f3205
DateReviewed: 2024-05-18
Description: Kubernetes is great, but performance may not be, ugh. So this article will look into K8s performance. To get good performance, we need to know what is going on, and the best way to do that is metrics. K8s gives us a few types - Node, pod and cluster level metrics, and the article does a couple sentences on each. Metrics give us insights they point out, like showing bottlenecks, detecting issues, helping to optimize costs and even scale based on demand. They now show us a method to do scaling, with YAML, to match demand with resorces; they use a CPU utilization target to adjust how many replicas are running. Next up, we look at health and performance stats, and we do so with a small Python program. In this case they just print them out, but we could say alert based on finding issues or anomalies. We then pivot to resource requests and limits, which provide guardrails for what a paticular pod can use, with YAML example. Next, they talk about persistent volumes and monitoring, but only include a single CLI command to display them. We are then shown how to do custom metrics, again with an example Python script to get us going. We then go through some handwaving at scaling strategies based on metrics and resolving bottlenecks, but both of these get a couple paragraphs when they are their own deep dive each. Our last topic is continuous improvement through metric analysis, which is a great approach but again covered in just two paragraphs. It's a great intro to metrics but tries at points to be hands on in parts, but lacks depth in important areas.
BottomLine: Good as an intro to Kubernetes metrics but tries and fails to be more
==
URL reference: https://www.infoworld.com/article/3715402/grafana-shining-a-light-into-kubernetes-clusters.html
DateReviewed: 2024-05-18
Description: Kubernetes is complex, and this article is a bit about the DevOps journey to create a tool that helps ease the complexity of K8s by visualizing it - Grafana. We start with a history lesson; first a bit of the DevOps parts, and what caused the birth of Grafana, and then a bit of the rise of cloud computing, as inexpensive Linux servers started to rise in the data center. There is a bad analogy that K8s is the "cloud operating system", which they say brings quibbles, which they try to justify with another analogy. We then pivot to what Grafana is good at - dashboard creation and visualizaion. They cite user successes, and allude to it's key value prop - bringing understandable data to the masses. They then talk about how plugins were key to bringing users on, naming important early ones like Elasticsearch and Prometheus and lauding the project as open source. They then talk a bit about the future, and how instead of building dsahboards, through understanding the data the dashboards might almost build themselves. They also touch on "de-aggregation", which is being able to drill down from a chart to see the source data -- which is shades of what Tableau was famous for. There is even a handwave to AI and what it might bring in one paragrpah. They also talk about the mindset of "you wrote it, you run it", which is about owning your code. Okay, so it's quite high level, but an enjoyable read with lots of insights.
BottomLine: Journey through the history and appraoch of Grafana for Kubernetes
==
URL reference: https://outshift.cisco.com/blog/inject-secrets-kubernetes-pods
DateReviewed: 2024-05-19
Description: This article deals with an alternate way to get sensitive info into Kubernetes instead of using K8s secrets. So they start by talking about a few variations of vaults that let you inject secrets, unlike K8s secrets. The way they do this is by injecting the secret at pod creation time, and this article talks about how they have now automated it. It uses a webhook to get the secret into the pod, and the updated automated approach rebuilds the webhook if a secret has changed, making sure the right version of the secret is available. They do have an extensive walkthrough, but almost no discussion on how this is better than the K8s native secrets.
BottomLine: Intro and walkthrough of using a vault in Kubernetes for secrets
==
URL reference: https://ubuntu.com/blog/ubuntu-pro-for-eks-is-now-generally-available
DateReviewed: 2024-05-19
Description: This article is the announcement of Ubuntu Pro for AWS EKS. So we start with lots of lauditory bits on the announcement and availability, and how this is wonderful. The OS is based on the Ubuntu minimal LTS image, and it seems to come with 12 additional months of support. The only real interesting bit here is FIPS and FedRAMP support, which I think means it's suitable for US government uses. Amazingly, not really too much of why this is better for EKS, and absolutely zero on what Ubuntu Pro is - there is a "link to learn", but write a couple sentences in your announcement, come on.
BottomLine: Awful announcement of Ubuntu Pro for EKS - AWS Kubernetes
==
URL reference: https://itnext.io/how-to-write-a-kubernetes-operator-5f98f7ef9f75
DateReviewed: 2024-05-20
Description: This article details a dev working on a Kubernetes operator. We start with an interesting backstory of the author, and how he squeezed the time in to do this project. We finally get to the operator, and he shows us the initial CLI steps and explains what he is doing in the text. He is using kubebuilder for this, and creates a backup and db-backup API for the operator. He takes us through some code, which I think is Go, before hooking things up in YAML to test. He then applies the various YAML bits to create a test DB and the operator to do the backups. There is a link in the end snip to the repo. It's entertaining, but I'd have loved a bit more of a dive on the operator pieces.
BottomLine: Interesting story and walkthrough on creating a Kubernetes operator
==
URL reference: https://dev.to/spronin/from-zero-to-hero-disaster-recovery-for-postgresql-with-streaming-replication-in-kubernetes-2g96
DateReviewed: 2024-05-20
Description: This article is a look at streaming replication on PostgreSQL. We start with a design (and a diagram!). They describe theh layout, which is 2 regions each with a primary and a replica. It's a hand on, so they take us through some prereqs and give us a link to the repo. We get a snip where we are exposing the main cluster so it can be connected to - in this case, via a ClusterIP. They explain how to do client/server Auth by TLS, and explain and link to how to do it. So, we have the main cluster, and now we have a standby, and they give us some YAML to config the standby correctly. To verify things are working, they say we should insert into the main cluster and then query for that data in the standby. Very easy and pretty simple, actually.
BottomLine: Walkthrough of setting up a DR PostgreSQL cluster on Kubernetes
==
URL reference: https://medium.com/@supportfly/how-to-resolve-kubectl-unable-to-connect-to-the-server-34e5605156e7
DateReviewed: 2024-05-21
Description: We start with an error, and with this article, are going to figure out how to solve it. It seems the "unable to connect" error is generally kubectl telling us it can't talk to the cluster. They give a list of possibilities, and it looks like we're going to tackle each one. So there are 5 issues they zero in on -- each has a diagnosis section, then a solution section. They walk through how to test this might be the issue, and then how one might fix it. They then have an additional tips section, where they suggest looking at the logs, checking for a version mismatch, and finally looking at a verbose output of the pods; all for clues of what might be wrong. If you think we are done, WRONG, because wierdly they now have a FAQ section which covers some of the issues we went through already?
BottomLine: Good early part that tackles connection issues to Kubernetes from kubectl
==
URL reference: https://earthly.dev/blog/openshift-vs-kubernetes/
DateReviewed: 2024-05-22
Description: This article takes a look at Kubernetes and OpenShift through the lens of a CI/CD system. We start with two "what is" for both K8s and OpenShift, just a paragraph on each for those that are familiar. They then go through the strengths of each; OS gets the nod with an easy GUI system, which makes getting started fast, where K8s is all CLI based which makes it more flexible and scriptable. Out of the box, OS has a component included for CI/CD, where K8s needs something like ArgoCD to get there. Surprisingly, for install K8s has lots of ability to be used on a variety of systems, while OpenShift is RHEL/CoreOS specific. They also point out OS has commercial variants, while K8s is open source first. We transition to security; they point out OS is more picky about what image repos can be used, where K8s doesn't have those restrictions by default. OS has A&A built in, while K8s needs RBAC and policies to get this outcome. OS also uses security contexts to control pod abilities, and this requires config to do on K8s. Lastly we go through updates - K8s does this easily out of the box, but for OS there is a manual process needed for major upgrades. I like this topic, but honestly this feels heavily weighted towards OS in this article, which isn't a bad thing but I think goes against the "vs" in the title. Main takeaway is for CI/CD OS might be easier to roll out if you don't mind RHEL/CoreOS, but if you need diversity you might need K8s.
BottomLine: Interesting runthrough of strengths of OpenShift compared with Kubernetes
==
URL reference: https://nws.netways.de/blog/2023/09/05/l7-traffic-filtering-with-cilium/
DateReviewed: 2024-05-23
Description: Filtering L7 traffic apparently helps security, so we'll do it with Cillium on Kubernetes in this article. It's a hands on, so you need a K8s cluster to do this, and they start with applying a URL with YAML in it; very secure there. They do explain what it does, namely creating some routes which are private and public for testing. So next up is testing it out, so they use curl to snag a few different interactions. They want to restrict things, so a bit of YAML later, we have restrictions. They test those, and impose more restrictions (again through YAML). So, I like the idea, but there is a lot you might be able to do, and using it just as what seems like a fancy firewall is not it. But it's a solid walkthrough.
BottomLine: Walkthrough of using Cillium to restrict Kubernetes traffic at layer 7
==
