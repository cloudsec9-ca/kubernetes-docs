URL reference: https://vahid-r.com/posts/kubernetes-passwords/
DateReviewed: 2024-03-01
Description: Kubernetes is great, but security is blah because of .. hard coded passwords in Helm, is how we start. So we dive in, and they observe that usernames and passwords are encoded in base64, but they are easily decoded. They give a few ways to avoid this, including K8s secrets, external config, Helm values and even encrypted secrets. I think the big takeaway here is not to let your secrets be committed to a repository, and to use the most secure method to hide them you can.
BottomLine: Good discussion of password issues in the context of Kubernetes and Kubernetes tools -- Helm in this case
==
URL reference: https://aws.plainenglish.io/how-to-get-versions-of-all-your-add-ons-in-aws-eks-cluster-15486e1238fd
DateReviewed: 2024-03-02
Description: Here we are going to look at the managed K8s service EKS from AWS - paticularly, the add-on part of it. We dive right in to understand add-ons, which are thing like networking solutions or monitoring tools that build on the base K8s functionality. This is a hands on, so you need an account and the right perms; we start with listing the add ons with the right CLI command. We then dive deeper with another command to grab details. They also show how to query using Helm and ConfigMaps. Actually an interesting topic.
BottomLine: Good intro to AWS EKS and how you might manage versions of add ons you might install
==
URL reference: https://www.linkedin.com/pulse/maximize-kubernetes-security-automate-tls-certificate-management-jatoc/
DateReviewed: 2024-03-03
Description: TLS and SSL are important in securing K8s comms, so we have to worry about how we issue certificates; this article focuses on that, using KIND and cert-manager. We dive right in with short intros to cert-manager then KIND. We then jump to the CLI, and spin up a KIND cluster, and then use CRDs and Helm to prepare for the install cert-manager and the jetstack repo. We do some Helm housekeeping and install cert-manager. We then go through the process of setting up automatic certs via a ClusterIssuer with YAML to cert-manager. We then tour another YAML to generate an actual cert. We then go through the process of setting up ingress and having certs created automatically that way, and thereby allowing external communications to the cluster. Again, more YAML is shown with explainations on how things work. Actually an interesting walk through.
BottomLine: Walk through on automating certs in Kubernetes
==
URL reference: https://rajeshgheware.medium.com/navigating-the-shift-mastering-pod-security-in-kubernetes-0d4df9bf73e1
DateReviewed: 2024-03-04
Description: We start with Kubernetes is rapidly evolving and security remains   a big concern. The article will focus on one aspect of security, namely Pod security admission (PSA), the successor to PSP or Pod Security Policies. We dive in to a quick background of what PSA does, then talk about how it works with its three predefined levels - privileged, baseline and restricted, with a sentence on each. Now onto how to make it work, and a bit of hand on. We travel through a three step process with YAML and CLI commands, activating PSA, defining Namespace labels and finally configuring the level we want to apply to the namespace. They then go through a quick example, with a restricted manifest. They explain the advantages and strategies for adopting PSA before wrapping up. Short but a good exposition and approach.
BottomLine: Good intro and walkthrough of Pod Security admission on Kubernetes
==
URL reference: https://cloud.google.com/kubernetes-engine/docs/deploy-app-cluster
DateReviewed: 2024-03-05
Description: This is a tutorial on how to get a basic app working on Google Kubernetes Engine, or GKE. We dive right in with the pre-reqs, which is setting up a Google account. They then walk through creating a project and enabling API access. Then then show how to set up cloud shell and configure so you can access your project. Next up they spin up a GKE cluster, and then grab the auth details needed to admin it through kubectl. We are now in Kubernetes, so next they create a deployment with a CLI command and explain it. Then they expose the deployment, so you have access from the Internet. We then verify our app is running in a pod and check the deployed service seems okay. Finally we can surf to the target IP. To conclude, they show how to clean up the service in K8s and also how to delete the cluster to not incur charges. The have an optional bit, where they show you the code, in what I think is a way to allow you to see and modify a bit of code that is K8s deployable.
BottomLine: Good to the point walkthrough on setting up a Kubernetes cluster on GKE
==
URL reference: https://www.adityasamant.dev/post/users-groups-roles-and-api-access-in-kubernetes
DateReviewed: 2024-03-06
Description: Our article is a walkthrough of K8s RBAC and some kubectl params. Prereq is a Kubernetes cluster, and we start with a sentence on what RBAC is and a link for more details. We start with a kubectl command to verify the auth we are currently using. We go through some CLI, where we can see the auth details. We then talk a bit about certs, because that is how K8s authenticates things, and we go through parts of a cert and the K8s API and what they mean. They then issue some kubectl commands to see what the current identity is able to do. Next, up, we create a key and a certificate signing request, and then get K8s to sign and give us a certificate we can use for identity. We then use RBAC through kubectl to create permissions based on the identiy we created. We then walk through the difference between the user and as parameters to kubectl, before doing some work with them. We use the "as" parameter to verify permissions, which are as we would expect. They then show how "user" fails without local setup, and then proceed to do that setup, which means getting the right cert, and setting that as the credential for the user and then setting context so it can be used. They verify again with user, and all works. They show that a non-existant user doesn't throw an error when checking perms, they just don't have any permissions to do anything. They then go on to create another few users, one of which is a "superuser" and as a power user. They illustrate that using "as" doesn't pull in groups, so while our poweruser has a bunch of permissions they aren't properly reflected when we query with the "as" parameter, so we have to add "as-group" to get the correct result. Overall, a wonderful tutorial.
BottomLine: Great hands-on walk through of creating and accessing Kubernetes through users and RBAC
==
URL reference: https://cloudnativenow.com/features/kubernetes-is-gaining-momentum-but-security-still-lags-behind/
DateReviewed: 2024-03-07
Description: We start with Kubernetes is growing in usage, but security is not keeping up -- what a surprise here. They then talk about how containers isolate things from the developer perspective, but this doesn't make them immune to security issue. So they argue that visibility is essential to understanding your network. They postulate that it's running K8s in the cloud that is the challenge, and that on-prem had logging that helped with this. They then talk about visibility gaps, without really explaining what isn't being observed. Another point they make is that some attacks might be thwarted, but they aren't being reported, so the security team can't respond. Then they talk about the possibility that developers might miss something that the security team easily sees, because of a different focus. We then pivot to solutions, which include training, network segmentation, RBAC, and monitoring. They make a good point about attack simulation and knowing your app enviornment. Most importantly, this is a group effort including managemnent and good communications is the key. While it ends up strong, some of the motivating points are on the weak side, but the solution side is decent.
BottomLine: Good overall high level Kubernetes security message that makes some weak points at times
==
URL reference: https://medium.com/beekstech/do-linux-containers-suffer-any-performance-penalty-compared-to-physical-servers-6ee9fd8c69c9
DateReviewed: 2024-03-08
Description: We start with a biz problem we are thinking of tackling with K8s. One issue to address is does Kubernetes impose a performance overhead? That's what this article will address. We start with the background of virtual machines and what they impose, and walk through CPU improvements that helped with these penalties. We then contrast VMs with various isolation technologies provided by the OS, and how they impose very little overhead. We then focus on modern containers, and how they build on the previous OS isolation tech to achive their goals. So, they argue, there is no penalty, as the OS maintains all of this regardless of whether you are in a container or not. They then do a comparison between network bandwidth in a container and on the base OS, with little difference. They do acknowledge that this is a limited test, but their conclusion is that there isn't too much of a variation.
BottomLine: Good theorectical comparison between native OS and container performance
==
URL reference: https://www.infoworld.com/article/3713286/kubernetes-is-not-a-cost-optimization-problem.html
DateReviewed: 2024-03-09
Description: While Kubernetes has become the default way to run microservice and like applications, it apparently is too expensive to run. But this article argues that it is not K8s that is too expensive, but the way we build applications. In the beginning, K8s was seen as a way to reduce VM usage, which would bring costs down, but many say that K8s itself got expensive. They argue that our paradigm changed, and we started building leaner micro apps, as well as K8s itself getting more sophisticated. They argue that by making running copies of a paticular app cheap, and for reliability/scalability reasons running 3 copies but often 5, we actually consumed more net resources. This assures a single node dying doesn't affect the service too much, but also that a spike in load can be distributed to physically different servers. They further argue that due to slow starting time, we often run apps overprovisioned, just in case, even with autoscaling. Another contributor is sidecars, which are helper containers that run alongside our main container. So, the answer to this is -- serverless and WebAssembly, it seems. They claim that capacity matches demand, because only when there is a request is capacity actually allocated. They do concede not everything is a good fit for serverless -- things like databases are better run in a long-lived system, like a container or a VM. But they make a good case that many other apps could be structured as container apps.
BottomLine: Interesting thoughts on Kubernetes costs and alternatives
==
URL reference: https://medium.com/@jleonro/mitigating-security-risks-on-kubernetes-on-aws-4b308534c4b6
DateReviewed: 2024-03-10
Description: We start with no interlude and jump right into the content, with a diagram of how Kubernetes is typically set up. Control plane, nodes, registry, and biz logic. On a second diagram we have little hacker icons pointing at many of the areas of Kubernetes. So, let me pause here and say that I thought this would be a serious article, but let's go forward and disabuse you of that thought. If we use AWS, then the control plane is run by the provider, so we are free there. And if we use Fargate, AWS also picks up responsibility for the node parts. Oh, and if you want the final bit delegated, you can use AWS Lambda. Starts interesting, but THAT is the entire article. No talk about costs, pros or cons -- just delegate away all of your security concerns, no worries. It's magic -- except for all of the caveats and costs.
BottomLine: Basically use AWS Kubernetes services if you don't want security responsibility
==
URL reference: https://thenewstack.io/7-great-tools-for-your-platform-engineering-toolchain/
DateReviewed: 2024-03-11
Description: We start with Platform engineering, which is supposed to integrate traditional development with DevOps; but this has led to many hype-filled tools. Our article will dive into ones that live up to their hype. We start with OPA and pair it with Rond - which are a general policy engine mixed with a container that helps implement those policies. We get 4 paragraphs that do a good high level job of explaining how they work together. Next is HashiCorp's Vault, which provides secrets security, and it gets the same 4 para treatment. Next up are another pair, Prometheus and Grafana - a logging and visualization pair, meant to surface the state of your cluster. Next is kube-green, which aims to reduce the cost and footprint of your cluster. After that we look at Git providers, which is any service that lets you run git, like GitHub or Bitbucket. Next up after source control is building, and we take a look at ArgoCD for the build process. Last up is Mia-platform, which allows you to manage your internal developer platform and provide better developer experience. We then dig into some key considerations for tools, and then look into where the future lies for these type of tools. Actually a good high level survey of this space.
BottomLine: Does a good job with a high level look at 7 cloud native development tools
==
URL reference: https://medium.com/@haticeadiguzelen/ubuntu-kubernetes-installation-a855d2bd4a4e
DateReviewed: 2024-03-12
Description: We're going to look at installing the main K8s cluster CLI control tool kubectl. We jump in and start in the shallow end by downloading and making the binary executable. We even check it matches the fingerprint (kudos here). We then put it in the main bin directory and verify output from the tool. So we've installed the tool, but now we need a cluster -- Minikube to the rescue. We then download Minikube, check the version, and start it. We can now use our installed kubectl to control our minikube cluster. They then go on to talk about using Docker or another container provider to run things with Minikube, and finally they talk about kubeadm and how that works. A good beginners guide to getting your toes wet.
BottomLine: Beginner walkthrough of a simple Kubernetes cluster
==
URL reference: https://nirmata.com/2024/03/04/preventive-security-vs-detection-and-response/
DateReviewed: 2024-03-13
Description: This article is going to look at proactive or preventative security measures and contrast that with reactive strategies like detection and response. We start with preventive measures, which are framed as preventing misconfigs. They do a short paragraph of background, and then a few points on benefits and limitations. Next we're on to detection and response, which they frame as runtime security. Another short background with the same pros and cons followup. They then identify 3 points of contrast, writing a paragraph on each of these. Next they have a 3 paragraph discussion of Admission controllers as policy enforcement in the preventive camp. They conclude saying you need a bit of both strategies to get a good coverage.
BottomLine: Good high-level look at two constrasting security strategies for Kubernetes and Cloud Native deployments
==
URL reference: https://www.akamai.com/blog/security-research/2024/mar/kubernetes-local-volumes-command-injection-vulnerability-rce-system-privileges
DateReviewed: 2024-03-14
Description: Kubernetes popular, but security is how we start -- but surprise surprise, this one is for K8s Windows. I don't have much exposure to Windows based K8s, so let's see what this article has to say. So there is a paticular config setup required for this vulnerability, and there is a bit of a journey to get ther. We start with a primer on K8s volumes, the thing that gives you storage. We then go through a tour of command lines, and paticularly how we can chain commands, through the "&&" operator. Now we pivot back to K8s vols, this time in the form of persistent volumes. Persistent volumes allow containers to "claim" them, and it seems we can craft the mount point such that we can add trailing bits to create a new command when the persistent volume is set up in the container. The K8s team moved from an OS callout to a native funtions, which eliminates the chaining vector. They point out again this is only a concern in Windows versions of K8s, and is patched in 1.28.3. So, how big a deal is this? Honestly, not as big as I think it seems. This is all controlled in the YAML setup, so unless you are running multi-tennant or running K8s as a service, the people controlling how pods are deployed could just set up a malicious pod with less hassle. It's not a remote execution or a container escape, just a sloppy bit of code. I'd also say K8s on Windows is far less used, so that factors in as well. But it's good that it was found and fixed.
BottomLine: Interesting security issue with Volumes on Kubernetes for Windows
==
URL reference: https://medium.com/@Coredge_79865/kubernetes-lifecycle-management-in-a-multi-cloud-environment-best-practices-6ae46a1d4b1f
DateReviewed: 2024-03-15
Description: Tackling lifecycles, the article introduces some terms off the top, including day 0 or design, day 1 or deploy, and day 2 or operate. They then touch on multi-cluster envs, and how they don't have to be multi cloud or multi region, but those are attractive things that do bring various benefits. Now we start to go into the reasons you'd want multi cluster - reduced latency - your workloads can be closer to your users; availability - if one cluster fails, then you can shift the work to another cluster; scalability - being able to increase your capacity depending on workloads. They then talk about workload isolation, but how that can be done with multiple clusters but also with K8s namespaces. The argue that it enhances flexibility, as you can fine-tune your cluster wide defaults differently on you different clusters. They further argue clusters can be more secure, and it might be easier to obtain compliance in this environment. While this is an interesting discussion, they actual case for multiple clusters is harder to understand here for me.
BottomLine: Discussion about benefits of using multiple Kubernetes clusters
==
URL reference: https://weng-albert.medium.com/understanding-the-basics-of-internal-networking-in-kubernetes-dfc27beef60d
DateReviewed: 2024-03-16
Description: We start by being told this is the author's reflections of K8s networking, so let's dive in. We start with a basic list of areas we'll cover and there are 7, so this should be interesting. We start with the basics, where he covers the basic K8s ground rules - No pod to pod comms need NAT, even on different nodes, and a pod should be at the address it thinks it is. There are lots of diagrams here, and our next topic, container to container loops one in. We talk about network namespaces, ip assignments and traffic flow. Nest up is pod to pod, which uses full on IP networking and addresses, even if on the same node. They take us through traffic flows, both on-node and between nodes. We then move onto pod to service, because pods are supposed to be ephemeral. Services get a static IP, which is then shared virutally and balances traffic. They then dig in and explain how the traffic is routed and use some diagrams to illustrate. We move onto an interesting one, external to service - where we have outside requests to our cluster. Both egress and ingress is covered, for outbound and inbound traffic, with step by step traffic walk-throughs. We move on to service discovery, explaining how it happens. Lastly, they explain how the type of a service works and what they mean. They cover ClusterIP, LoadBalancer and NodePort. 
BottomLine: Practical walk through of a number of Kubernetes networking concepts
==
URL reference: https://dzone.com/articles/understanding-the-cloud-security-landscape
DateReviewed: 2024-03-17
Description: We start with Kubernetes good, security bad -- what a novel concept. We then dive into some stats, including a report that less then a quarter of orgs say they have full visibility into their clouds. They also point to orgs using 3 to 6 different tools, which complicates understanding. They argue that gaps are created between dev and security teams, due to comms and other challenges. We now pivot to best practices and solutions. They say a unified security framework help to streamline things, and point out some of the things it makes easier. They also advance Zero trust architecture as a way to build out things, where you are always verifying different pieces of your solution. Another thing they touch on is encryption everywhere, where sensitive information is encrypted whenever it isn't actively being used. Lastly, they argue for continuous secrity compliance monitoring, arguing this helps to cement security in your system. My take is that this is a super high level document, with lots of great broad strokes but not much on what or how to implement. Not a beginner document, but rather a read for someone who has the basics elsewhere.
BottomLine: Interesting read from a very high level on Kubernetes security approaches
==
URL reference: https://itnext.io/cluster-dev-is-it-the-right-kubernetes-bootstrapping-tool-for-you-6dd5e92aa9a0
DateReviewed: 2024-03-17
Description: This article looks at dev services through a tool called Cluster.dev. The tool builds intfra stack templates like Helm and Terraform, and we're going to do a little tour. They do stress the tool does more than what they cover, and that they aren't experts in the tool. The tool has a concept of units, which they say parallels modules in Terraform, which you use like Lego blocks to build something out of smaller parts. So you use units to build stacks, and this can be used to test and deploy your entire enviornment. They pop a diagram and a handy reference or two at this point in the discussion. They then pivot to contrast and compare with various tools like Helm and Terraform, among others. Each one gets a full paragraph to go through and explain differences and strengths. They then do a walk-through of spinning up a K8s cluster on Azure, with all of the commands. Overall an interesting tool.
BottomLine: Background and short walk through on Kubernetes infra tool cluster.dev
==
URL reference: https://medium.com/@mkaran2805/psp-to-psa-what-changed-2774193dc2ed
DateReviewed: 2024-03-18
Description: We start with the fact that there was a big shift over the K8s version lifecycle of 1.24, 1.25 and 1.26, and this article will go through that. They start with .24, which marked the removal of the old PodSecurityPolicy (PSP) and it's replacement by the PodSecurity Admission {PSA) mechanism. For .25, there were some enhancements to PSA, along with the introduction of Windows Privileged containers, mirroring what Linux had already had. In .26.5, there were yet more fixes and enhancements to the PSA, as well as the introduction of Security context deny admission controller as an alpha-quality tool. The latter enforces that pods mush have a security context to run. Short and sweet -- but I got the impression there would be a bit of a comparison between the two Pod methods, but no.
BottomLine: High level walkthrough of the timeline of changes from PSP to PSA for Kubernetes without any discussion of the substance of the changes
==
URL reference: https://ubuntu.com//blog/try-canonical-kubernetes-beta
DateReviewed: 2024-03-19
Description: This is about the intro of a new Kubernetes distro from Canonical. Starts with the normal stuff about K8s being great, then talkea a bit about Canonical's background with K8s. They have supported MicroK8s and Charmed Kubernetes, and now they are going to provide a new third distro focused on ZeroOps - Canonical Kubernetes. It uses snap (sigh), so is easy to install - you install it and then run bootstrap to start it up. They then detail what is included and running by default, namedropping things like Cilium and OpenEBS. They then walk through it appealing to everyone - prod servers, CI/CD pipelines and even dev workstations. Next they run through the security features -- with isolation built in and updates from snap. It's now in beta to try, just in time for KubeCon EU.
BottomLine: New Kubernetes distro announced from Canonical
==
URL reference: https://peilunl.medium.com/monitoring-availability-for-serverless-applications-on-kubernetes-ab879ba79137
DateReviewed: 2024-03-20
Description: This article is going to look at serverless and the way Kubernetes has embraced it, and examine what this means for logs and observability. They explain that we often use things like health checks or probe checks to assess target goodness, but if they are sleeping this won't be a great measure. We are told that it might be possible to infer health through watching real traffic, but this can also be problematic, as it means having an inspection layer which might have to be bespoke and need lots of tuning. They argue we should get what we need from deployment networks. They argue by watching for scale up from zero and further scale up events, coupled with pod level probes we can get our assurance that things are working as they should be. It also simplifies testing, because we can just look for running servers being less than requested servers for more then an amount of time -- they use 3 minutes in their example. They discuss various issues with alerts and states, including one tool not being able to alert on logical operators (only numbers or simple arithmetic). They use a simple but effective way to get the desired value out of the math they do. Interestingly, they also disclose they used AI to help with the article, including the full disclosure of the prompts they used -- I like it.
BottomLine: Good walkthrough on how to monitor serverless Kubernetes and a good discussion of why it works
==
URL reference: https://sysdig.com/blog/resource-constraints-in-kubernetes-and-security/
DateReviewed: 2024-03-21
Description: We start with Kubernetes security is concerning, from a research report. The approach here is that unchecked resources themselves can be a security issue. Their arguement is if compromised, unbridled containers can be used to mine crypto or other resource-intensive things. They then show us some YAML and explain that setting limits is actually fairly easy, and walk us through the various limits set in the YAML with explainations. Next we run a CLI command to view constraints, and they show where they end up. They also explain what happens if containers exceed their limits. They then use a stress container to test things out, and show the limits being enforced. They then show how to set up alerts for pods without constaints, so you know if a stray pod launches on your cluster. Then there is a bit of an ad, along with a few paragraphs advocating for more proactive security practices like contraint setting.
BottomLine: Great motivation and walk through of using Kubernetes contraints to limit your running containers
==
URL reference: https://dev.to/azure/strengthening-the-secure-supply-chain-2el4
DateReviewed: 2024-03-22
Description: This post is based on a conference talk, so there isn't too much motivation before we jump in, and I had low expectations from the title, but wow is what I'll say. We start with a list of prereqs, none of which should be out of reach for a developer. He start by installing KIND and kubectl, so we can have a Kubernets cluster to work with. He then spins up the cluster, and we move onto to installing the github CLI along with doing a bit of config setup. We then fork/clone the repo and then set it as the default. Next we build the sample app from the repo, and once built we push or image up to the image repository. We then install a tool called Kustomize, and use it to update some YAML for the sample app. He then installs Flux, does some setup and bootstraps it for the GitOps pipeline. So far, this has just been getting the tooling set up; now, he pivots to the secrity bits. So he grabs Trivy and installs it, and does a scan. The scan uncovers some issues, which we need to fix. To do this, he grabs Copacetic and install it, and then uses it to patch the security issues detected in our image. He then re-runs the Trivy scan and it comes out clean. Not satisfied with that, he then shows us how to automate the process, by configuring Flux to patch the image when updates are available, so we are always at the most recent rev of the dependencies. He even shows a tool called Eraser which can be used to delete deployed Kubernetes nodes that have security issues in then. He then shows us how to tidy up our local cluster, leaving it without any stray stuff. Honestly, a great approach and well done.
BottomLine: Awesome hands-on walkthrough of automating security in Kubernetes image dependencies
==
URL reference: https://logrhythm.com/blog/how-to-monitor-kubernetes-audit-logs/
DateReviewed: 2024-03-23
Description: Starts with the entirely new concept that K8s is hot, but security is not (sarcasm). They start from a 101 aspect, walking us through "what are containers and K8s". They then explain what K8s audit logs are, and why we may need them (security and compliance). They do a few paragraphs to argue how audit logs make our jobs easier. To my pleasant surprise, they dive into different aspects that are surfaced by various logs, and have a paragraph or two where they explain what each does and how they help. There are also a number of good diagrams throughout the piece. Ah, and we now have the promoted section, where the vendor explains how to ingest logs into their product. They then go on to explain how Audit policy works with regards to events.
BottomLine: Decent but high-level approach to Kubernetes security
==
URL reference: https://dev.to/refine/the-basics-of-kubernetes-cronjob-42k3
DateReviewed: 2024-03-24
Description: We start with a basic handwave about K8s, and how periodic jobs are useful, like the Linux built-in cron utility. So, this article will cover how to set up this functionality in Kubernetetes. We start with a small explaination of how CronJob in K8s differs from the Unix cron - specifically that K8s CronJob can leverage K8s stuff like secrets and scaling where cron is limited to single machine visibility. They have some pre-reqs and go through a quick setup, to get things configured. Next up is creating a CronJob, which of course means creating a YAML to control it. They then, with diagrams, break down what the YAML is doing and how to both run and verify your YAML for the CronJob. Next up they go through examining logs for debugging, because of course things sometimes go sideways. They then wrap up with some real world use case examples at a high level, and complete it with some best practices.
BottomLine: Good walkthrough of setting up CronJob functionality under Kubernetes
==
URL reference: https://medium.com/@s4saif.121/automation-of-vms-on-ocp-virtualization-using-ansible-automation-platform-under-5-minutes-31ce494581c4
DateReviewed: 2024-03-25
Description: We dive right in, first with a diagram then with a short background on what OpenShift virtualization is. Basically it's a way to run VMs alongside containers, for those who have that need. Next up is a high level intro to Ansible, the distributed automation platform. Now we jump into a hands on, where we'll deploy a centos7 VM with an web server configured during boot. We start with a YAML file, which gives us the basics of our VM and what we need to get going. A second YAML file sets out how the deployment should actually happen. They go through a number of YAML snips to do the various bits, and finally commits these all to git. They then pivot to the Ansible automation platfrom, a GUI that allows you to control Ansible, where they set up creds, create an Ansible project and finally run the code, called a playbook in Ansible terminology. You can watch the logs, and once it runs successfully, you can see your VM is running. 
BottomLine: Walkthrough of using Ansible and OpenShift to run VMs alongside Kubernetes
==
URL reference: https://leebriggs.co.uk/blog/2024/03/23/why-public-k8s-controlplane
DateReviewed: 2024-03-25
Description: We start with the question "do you think about your K8s control plane access?" The key item is that by default, many Kubernetes distros have their admin interface exposed on the open Internet. They contrast this with other services like databases and the like, which we don't put out there. He goes through why it might be, but I have a thought -- many people don't think about the admin side once it's up and running, and don't think of security implications. There is talk of bastions or VPNs, but our author is going to show us how to set up private access through Tailscale. First we need a tailnet, and that requires installing Tailscale, which he links to. Next we need a K8s cluster, and he links to a way to do that to. Now we start a walktrhough, where he uses Helm to install the K8s Tailscale operator, and then invokes tailscale to create a YAML config for K8s. We need to tweak a config to add credential info, and bang we're in. What I like is there are a few alternative configs in the FAQ area, if you want to set up your cluster in a different way. Also, the underlying message is get your Kubernetes admin pages off the public Internet!
BottomLine: Good walkthrough on how to access the Kubernetes control plane from a secured tunnel
==
URL reference: https://dev.to/choonho/install-cert-manager-lets-encrypt-443b
DateReviewed: 2024-03-26
Description: This one is straight into and a bit lean -- it starts by saying _certification_ is a critical safety feature but I think they mean certificates. We get a snippet to install cert-manager and then a YAML file to set up Let's Encrypt. No explaination of what it does, and actually issuing a certificate is left as an exercise to the reader. Now, I like thin articles, but you have to do all the heavy lifting with this one.
BottomLine: Too lightweight a look at certs and Lets Encrypt for Kubernetes
==
URL reference: https://cloudnativejourney.wordpress.com/2024/03/26/implementing-disaster-backup-for-a-kubernetes-cluster-a-comprehensive-guide/
DateReviewed: 2024-03-27
Description: Anything, including Kubernetes, can have failures; this article will examine setting up disaster recovery on K8s. The first section explains why we even need this, in 3 subparts - basically data protection, biz continuity and risk mitigation. Next up is an overview of the process, with all the terminology and parts drawn out in 5 sections. They also throw in some (3) additional considerations to think about, as well as a future trends section. While they outline at a 30 thousand foot level a comprehensive approach, I'm thinking why this way? While you 100% need backups of your k8s configs and all of your data, why aren't you trying to build something that can do this all the time, I mean. Spin up your cluster in a different region, change your various LB/DNS pointers and test and confirm it works.
BottomLine: Interesting walk through of a traditional disaster recovery approach on Kubernetes
==
URL reference: https://malisatish.medium.com/deploy-node-js-application-using-local-kubernetes-cluster-c65bf098bac0
DateReviewed: 2024-03-28
Description: We start with a quick statement of objectives - install Minikube, configure YAML file for deployment, and deploy the application. This is hands on, so first we downlaod and then install minikube off the net. For kubectl, we get fancy and download, validate and then install and finally test the version. All set up, we then fire up minikube. We are then shown a YAML file, which contains the config for the app. Apparently we need to load the image to minikube, and then we are shown the invocation to actually deploy the app. They then verify the app is in the pod list. So, it's a barebones walkthrough without any explaination -- so it is good to follow along, but you don't get real understanding. There is NO time spent on node.js -- it could be a binary in the image, we never interact with it.
BottomLine: Terse walkthrough of deploying an app to Kubernetes
==
