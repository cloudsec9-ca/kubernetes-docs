URL reference: https://vahid-r.com/posts/kubernetes-passwords/
DateReviewed: 2024-03-01
Description: Kubernetes is great, but security is blah because of .. hard coded passwords in Helm, is how we start. So we dive in, and they observe that usernames and passwords are encoded in base64, but they are easily decoded. They give a few ways to avoid this, including K8s secrets, external config, Helm values and even encrypted secrets. I think the big takeaway here is not to let your secrets be committed to a repository, and to use the most secure method to hide them you can.
BottomLine: Good discussion of password issues in the context of Kubernetes and Kubernetes tools -- Helm in this case
==
URL reference: https://aws.plainenglish.io/how-to-get-versions-of-all-your-add-ons-in-aws-eks-cluster-15486e1238fd
DateReviewed: 2024-03-02
Description: Here we are going to look at the managed K8s service EKS from AWS - paticularly, the add-on part of it. We dive right in to understand add-ons, which are thing like networking solutions or monitoring tools that build on the base K8s functionality. This is a hands on, so you need an account and the right perms; we start with listing the add ons with the right CLI command. We then dive deeper with another command to grab details. They also show how to query using Helm and ConfigMaps. Actually an interesting topic.
BottomLine: Good intro to AWS EKS and how you might manage versions of add ons you might install
==
URL reference: https://www.linkedin.com/pulse/maximize-kubernetes-security-automate-tls-certificate-management-jatoc/
DateReviewed: 2024-03-03
Description: TLS and SSL are important in securing K8s comms, so we have to worry about how we issue certificates; this article focuses on that, using KIND and cert-manager. We dive right in with short intros to cert-manager then KIND. We then jump to the CLI, and spin up a KIND cluster, and then use CRDs and Helm to prepare for the install cert-manager and the jetstack repo. We do some Helm housekeeping and install cert-manager. We then go through the process of setting up automatic certs via a ClusterIssuer with YAML to cert-manager. We then tour another YAML to generate an actual cert. We then go through the process of setting up ingress and having certs created automatically that way, and thereby allowing external communications to the cluster. Again, more YAML is shown with explainations on how things work. Actually an interesting walk through.
BottomLine: Walk through on automating certs in Kubernetes
==
URL reference: https://rajeshgheware.medium.com/navigating-the-shift-mastering-pod-security-in-kubernetes-0d4df9bf73e1
DateReviewed: 2024-03-04
Description: We start with Kubernetes is rapidly evolving and security remains   a big concern. The article will focus on one aspect of security, namely Pod security admission (PSA), the successor to PSP or Pod Security Policies. We dive in to a quick background of what PSA does, then talk about how it works with its three predefined levels - privileged, baseline and restricted, with a sentence on each. Now onto how to make it work, and a bit of hand on. We travel through a three step process with YAML and CLI commands, activating PSA, defining Namespace labels and finally configuring the level we want to apply to the namespace. They then go through a quick example, with a restricted manifest. They explain the advantages and strategies for adopting PSA before wrapping up. Short but a good exposition and approach.
BottomLine: Good intro and walkthrough of Pod Security admission on Kubernetes
==
URL reference: https://cloud.google.com/kubernetes-engine/docs/deploy-app-cluster
DateReviewed: 2024-03-05
Description: This is a tutorial on how to get a basic app working on Google Kubernetes Engine, or GKE. We dive right in with the pre-reqs, which is setting up a Google account. They then walk through creating a project and enabling API access. Then then show how to set up cloud shell and configure so you can access your project. Next up they spin up a GKE cluster, and then grab the auth details needed to admin it through kubectl. We are now in Kubernetes, so next they create a deployment with a CLI command and explain it. Then they expose the deployment, so you have access from the Internet. We then verify our app is running in a pod and check the deployed service seems okay. Finally we can surf to the target IP. To conclude, they show how to clean up the service in K8s and also how to delete the cluster to not incur charges. The have an optional bit, where they show you the code, in what I think is a way to allow you to see and modify a bit of code that is K8s deployable.
BottomLine: Good to the point walkthrough on setting up a Kubernetes cluster on GKE
==
URL reference: https://www.adityasamant.dev/post/users-groups-roles-and-api-access-in-kubernetes
DateReviewed: 2024-03-06
Description: Our article is a walkthrough of K8s RBAC and some kubectl params. Prereq is a Kubernetes cluster, and we start with a sentence on what RBAC is and a link for more details. We start with a kubectl command to verify the auth we are currently using. We go through some CLI, where we can see the auth details. We then talk a bit about certs, because that is how K8s authenticates things, and we go through parts of a cert and the K8s API and what they mean. They then issue some kubectl commands to see what the current identity is able to do. Next, up, we create a key and a certificate signing request, and then get K8s to sign and give us a certificate we can use for identity. We then use RBAC through kubectl to create permissions based on the identiy we created. We then walk through the difference between the user and as parameters to kubectl, before doing some work with them. We use the "as" parameter to verify permissions, which are as we would expect. They then show how "user" fails without local setup, and then proceed to do that setup, which means getting the right cert, and setting that as the credential for the user and then setting context so it can be used. They verify again with user, and all works. They show that a non-existant user doesn't throw an error when checking perms, they just don't have any permissions to do anything. They then go on to create another few users, one of which is a "superuser" and as a power user. They illustrate that using "as" doesn't pull in groups, so while our poweruser has a bunch of permissions they aren't properly reflected when we query with the "as" parameter, so we have to add "as-group" to get the correct result. Overall, a wonderful tutorial.
BottomLine: Great hands-on walk through of creating and accessing Kubernetes through users and RBAC
==
URL reference: https://cloudnativenow.com/features/kubernetes-is-gaining-momentum-but-security-still-lags-behind/
DateReviewed: 2024-03-07
Description: We start with Kubernetes is growing in usage, but security is not keeping up -- what a surprise here. They then talk about how containers isolate things from the developer perspective, but this doesn't make them immune to security issue. So they argue that visibility is essential to understanding your network. They postulate that it's running K8s in the cloud that is the challenge, and that on-prem had logging that helped with this. They then talk about visibility gaps, without really explaining what isn't being observed. Another point they make is that some attacks might be thwarted, but they aren't being reported, so the security team can't respond. Then they talk about the possibility that developers might miss something that the security team easily sees, because of a different focus. We then pivot to solutions, which include training, network segmentation, RBAC, and monitoring. They make a good point about attack simulation and knowing your app enviornment. Most importantly, this is a group effort including managemnent and good communications is the key. While it ends up strong, some of the motivating points are on the weak side, but the solution side is decent.
BottomLine: Good overall high level Kubernetes security message that makes some weak points at times
==
URL reference: https://medium.com/beekstech/do-linux-containers-suffer-any-performance-penalty-compared-to-physical-servers-6ee9fd8c69c9
DateReviewed: 2024-03-08
Description: We start with a biz problem we are thinking of tackling with K8s. One issue to address is does Kubernetes impose a performance overhead? That's what this article will address. We start with the background of virtual machines and what they impose, and walk through CPU improvements that helped with these penalties. We then contrast VMs with various isolation technologies provided by the OS, and how they impose very little overhead. We then focus on modern containers, and how they build on the previous OS isolation tech to achive their goals. So, they argue, there is no penalty, as the OS maintains all of this regardless of whether you are in a container or not. They then do a comparison between network bandwidth in a container and on the base OS, with little difference. They do acknowledge that this is a limited test, but their conclusion is that there isn't too much of a variation.
BottomLine: Good theorectical comparison between native OS and container performance
==
URL reference: https://www.infoworld.com/article/3713286/kubernetes-is-not-a-cost-optimization-problem.html
DateReviewed: 2024-03-09
Description: While Kubernetes has become the default way to run microservice and like applications, it apparently is too expensive to run. But this article argues that it is not K8s that is too expensive, but the way we build applications. In the beginning, K8s was seen as a way to reduce VM usage, which would bring costs down, but many say that K8s itself got expensive. They argue that our paradigm changed, and we started building leaner micro apps, as well as K8s itself getting more sophisticated. They argue that by making running copies of a paticular app cheap, and for reliability/scalability reasons running 3 copies but often 5, we actually consumed more net resources. This assures a single node dying doesn't affect the service too much, but also that a spike in load can be distributed to physically different servers. They further argue that due to slow starting time, we often run apps overprovisioned, just in case, even with autoscaling. Another contributor is sidecars, which are helper containers that run alongside our main container. So, the answer to this is -- serverless and WebAssembly, it seems. They claim that capacity matches demand, because only when there is a request is capacity actually allocated. They do concede not everything is a good fit for serverless -- things like databases are better run in a long-lived system, like a container or a VM. But they make a good case that many other apps could be structured as container apps.
BottomLine: Interesting thoughts on Kubernetes costs and alternatives
==
URL reference: https://medium.com/@jleonro/mitigating-security-risks-on-kubernetes-on-aws-4b308534c4b6
DateReviewed: 2024-03-10
Description: We start with no interlude and jump right into the content, with a diagram of how Kubernetes is typically set up. Control plane, nodes, registry, and biz logic. On a second diagram we have little hacker icons pointing at many of the areas of Kubernetes. So, let me pause here and say that I thought this would be a serious article, but let's go forward and disabuse you of that thought. If we use AWS, then the control plane is run by the provider, so we are free there. And if we use Fargate, AWS also picks up responsibility for the node parts. Oh, and if you want the final bit delegated, you can use AWS Lambda. Starts interesting, but THAT is the entire article. No talk about costs, pros or cons -- just delegate away all of your security concerns, no worries. It's magic -- except for all of the caveats and costs.
BottomLine: Basically use AWS Kubernetes services if you don't want security responsibility
==
URL reference: https://thenewstack.io/7-great-tools-for-your-platform-engineering-toolchain/
DateReviewed: 2024-03-11
Description: We start with Platform engineering, which is supposed to integrate traditional development with DevOps; but this has led to many hype-filled tools. Our article will dive into ones that live up to their hype. We start with OPA and pair it with Rond - which are a general policy engine mixed with a container that helps implement those policies. We get 4 paragraphs that do a good high level job of explaining how they work together. Next is HashiCorp's Vault, which provides secrets security, and it gets the same 4 para treatment. Next up are another pair, Prometheus and Grafana - a logging and visualization pair, meant to surface the state of your cluster. Next is kube-green, which aims to reduce the cost and footprint of your cluster. After that we look at Git providers, which is any service that lets you run git, like GitHub or Bitbucket. Next up after source control is building, and we take a look at ArgoCD for the build process. Last up is Mia-platform, which allows you to manage your internal developer platform and provide better developer experience. We then dig into some key considerations for tools, and then look into where the future lies for these type of tools. Actually a good high level survey of this space.
BottomLine: Does a good job with a high level look at 7 cloud native development tools
==
URL reference: https://medium.com/@haticeadiguzelen/ubuntu-kubernetes-installation-a855d2bd4a4e
DateReviewed: 2024-03-12
Description: We're going to look at installing the main K8s cluster CLI control tool kubectl. We jump in and start in the shallow end by downloading and making the binary executable. We even check it matches the fingerprint (kudos here). We then put it in the main bin directory and verify output from the tool. So we've installed the tool, but now we need a cluster -- Minikube to the rescue. We then download Minikube, check the version, and start it. We can now use our installed kubectl to control our minikube cluster. They then go on to talk about using Docker or another container provider to run things with Minikube, and finally they talk about kubeadm and how that works. A good beginners guide to getting your toes wet.
BottomLine: Beginner walkthrough of a simple Kubernetes cluster
==
URL reference: https://nirmata.com/2024/03/04/preventive-security-vs-detection-and-response/
DateReviewed: 2024-03-13
Description: This article is going to look at proactive or preventative security measures and contrast that with reactive strategies like detection and response. We start with preventive measures, which are framed as preventing misconfigs. They do a short paragraph of background, and then a few points on benefits and limitations. Next we're on to detection and response, which they frame as runtime security. Another short background with the same pros and cons followup. They then identify 3 points of contrast, writing a paragraph on each of these. Next they have a 3 paragraph discussion of Admission controllers as policy enforcement in the preventive camp. They conclude saying you need a bit of both strategies to get a good coverage.
BottomLine: Good high-level look at two constrasting security strategies for Kubernetes and Cloud Native deployments
==
URL reference: https://www.akamai.com/blog/security-research/2024/mar/kubernetes-local-volumes-command-injection-vulnerability-rce-system-privileges
DateReviewed: 2024-03-14
Description: Kubernetes popular, but security is how we start -- but surprise surprise, this one is for K8s Windows. I don't have much exposure to Windows based K8s, so let's see what this article has to say. So there is a paticular config setup required for this vulnerability, and there is a bit of a journey to get ther. We start with a primer on K8s volumes, the thing that gives you storage. We then go through a tour of command lines, and paticularly how we can chain commands, through the "&&" operator. Now we pivot back to K8s vols, this time in the form of persistent volumes. Persistent volumes allow containers to "claim" them, and it seems we can craft the mount point such that we can add trailing bits to create a new command when the persistent volume is set up in the container. The K8s team moved from an OS callout to a native funtions, which eliminates the chaining vector. They point out again this is only a concern in Windows versions of K8s, and is patched in 1.28.3. So, how big a deal is this? Honestly, not as big as I think it seems. This is all controlled in the YAML setup, so unless you are running multi-tennant or running K8s as a service, the people controlling how pods are deployed could just set up a malicious pod with less hassle. It's not a remote execution or a container escape, just a sloppy bit of code. I'd also say K8s on Windows is far less used, so that factors in as well. But it's good that it was found and fixed.
BottomLine: Interesting security issue with Volumes on Kubernetes for Windows
==
URL reference: https://medium.com/@Coredge_79865/kubernetes-lifecycle-management-in-a-multi-cloud-environment-best-practices-6ae46a1d4b1f
DateReviewed: 2024-03-15
Description: Tackling lifecycles, the article introduces some terms off the top, including day 0 or design, day 1 or deploy, and day 2 or operate. They then touch on multi-cluster envs, and how they don't have to be multi cloud or multi region, but those are attractive things that do bring various benefits. Now we start to go into the reasons you'd want multi cluster - reduced latency - your workloads can be closer to your users; availability - if one cluster fails, then you can shift the work to another cluster; scalability - being able to increase your capacity depending on workloads. They then talk about workload isolation, but how that can be done with multiple clusters but also with K8s namespaces. The argue that it enhances flexibility, as you can fine-tune your cluster wide defaults differently on you different clusters. They further argue clusters can be more secure, and it might be easier to obtain compliance in this environment. While this is an interesting discussion, they actual case for multiple clusters is harder to understand here for me.
BottomLine: Discussion about benefits of using multiple Kubernetes clusters
==
URL reference: https://weng-albert.medium.com/understanding-the-basics-of-internal-networking-in-kubernetes-dfc27beef60d
DateReviewed: 2024-03-16
Description: We start by being told this is the author's reflections of K8s networking, so let's dive in. We start with a basic list of areas we'll cover and there are 7, so this should be interesting. We start with the basics, where he covers the basic K8s ground rules - No pod to pod comms need NAT, even on different nodes, and a pod should be at the address it thinks it is. There are lots of diagrams here, and our next topic, container to container loops one in. We talk about network namespaces, ip assignments and traffic flow. Nest up is pod to pod, which uses full on IP networking and addresses, even if on the same node. They take us through traffic flows, both on-node and between nodes. We then move onto pod to service, because pods are supposed to be ephemeral. Services get a static IP, which is then shared virutally and balances traffic. They then dig in and explain how the traffic is routed and use some diagrams to illustrate. We move onto an interesting one, external to service - where we have outside requests to our cluster. Both egress and ingress is covered, for outbound and inbound traffic, with step by step traffic walk-throughs. We move on to service discovery, explaining how it happens. Lastly, they explain how the type of a service works and what they mean. They cover ClusterIP, LoadBalancer and NodePort. 
BottomLine: Practical walk through of a number of Kubernetes networking concepts
==
URL reference: https://dzone.com/articles/understanding-the-cloud-security-landscape
DateReviewed: 2024-03-17
Description: We start with Kubernetes good, security bad -- what a novel concept. We then dive into some stats, including a report that less then a quarter of orgs say they have full visibility into their clouds. They also point to orgs using 3 to 6 different tools, which complicates understanding. They argue that gaps are created between dev and security teams, due to comms and other challenges. We now pivot to best practices and solutions. They say a unified security framework help to streamline things, and point out some of the things it makes easier. They also advance Zero trust architecture as a way to build out things, where you are always verifying different pieces of your solution. Another thing they touch on is encryption everywhere, where sensitive information is encrypted whenever it isn't actively being used. Lastly, they argue for continuous secrity compliance monitoring, arguing this helps to cement security in your system. My take is that this is a super high level document, with lots of great broad strokes but not much on what or how to implement. Not a beginner document, but rather a read for someone who has the basics elsewhere.
BottomLine: Interesting read from a very high level on Kubernetes security approaches
==
